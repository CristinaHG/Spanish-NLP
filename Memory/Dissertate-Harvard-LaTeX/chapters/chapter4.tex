%!TEX root = ../dissertation.tex

\chapter{Material empleado}
\label{conclusion}
Anteriormente se mencionaba que los objetivos inicialmente previstos en este trabajo de PLN eran tres: desarrollar un método de tokenizado de texto, otro de POS etiquetado (etiquetado morfosintáctico) y otro de lematización usando el lenguaje \textsf{Scala}. Para ello, se toman como base de referencia las herramientas para Español (Pattern.es) de \textcolor{SchoolColor}{Pattern} \citet{smedt2012pattern}. 
\\[\baselineskip]
En este capítulo se detalla como se obtienen los datos que se han usado para la implementación de este proyecto pero que no forman parte de ésta, y que son importantes para el correcto funcionamiento de los algoritmos. Las implementaciones de los algoritmos se detallan en el capítulo siguiente. 
Vayamos detallando según las fases del proyecto: 
\section{Tokenizador}
\textcolor{SchoolColor}{Pattern} presenta un Tokenizador para español basado en reglas, 
que no necesita más que una cadena de texto a tokenizar.
\section{Pos Tagger}
Determinar el tipo de palabra que es cada token de un texto ya no es una tarea que pueda resolverse sólo aplicando algunas reglas consecutivas (al menos obteniendo resultados algo competitivos), dado que la función de las palabras varía según el contexto o su posición en una frase, la morfología...
\newline
El método propuesto en \textcolor{SchoolColor}{Pattern} para el tagger español consiste en usar un corpus etiquetado de palabras suficientemente grande y entrenar un algoritmo de machine learning que aprenda reglas y patrones presentes en esos datos. Concretamente, como corpus usa el \textcolor{SchoolColor}{Wikicorpus} \citet{reese2010word} para español, y como algoritmo usa un \textcolor{SchoolColor}{Algoritmo de Brill}:    
\subsection*{Wikicorpus}
Existen tres variantes para tres lenguas distintas: Español, Catalán e Inglés. El corpus en su versión actual cuenta con unas 750 millones de palabras en total (120 millones para el español) y está construido a partir de texto de la Wikipedia etiquetado con información lingüística, concretamente de lema y POS tag, usando la librería \textsf{Freeling} \citet{padro2012freeling}.
\subsection*{Algoritmo de Brill}
Este método fue propuesto por Eric Brill \citet{brill1992simple}. Es un algoritmo de aprendizaje también conocido como aprendizaje basado en transformación, dado que cada palabra recibe inicialmente una etiqueta que luego va modificándose a menudo que se aplican los pasos del algoritmo. Normalmente, la condición de parada es que alcance una tasa de error inferior a una obtenida por parámetro. También necesita recibir un corpus etiquetado como parámetro, ya que lo usará para el aprendizaje, un diccionario léxico al que a cada palabra le corresponde su tipo más común (verbo, nombre, etc), y una lista de plantillas de reglas. \newline
El funcionamiento es el siguiente:
\begin{enumerate}
\item Etiquetado inicial: El algoritmo comienza asignando a cada palabra su etiqueta más común, para ello la busca en el diccionario léxico que recibe como parámetro. En esta etapa se etiquetan mal muchas palabras, pues no se tiene en cuenta para nada el contexto de las mismas. Además, hay palabras que no estarán en el diccionario.
\item Etiquetar las desconocidas: \citet{padro2012freeling} propone dos procedimientos para ello. En primer lugar se considera que las palabras que no estaban presentes en el diccionario léxico y empiezan por mayúscula son nombres propios. En segundo lugar, se etiquetan las palabras desconocidas restantes asignándoles la etiqueta más común en palabras conocidas con las mismas tres últimas letras. Por ejemplo, \textsf{"roncaba"} se etiqutaría como verbo si \textsf{"cantaba"} fuera conocida.

\item Aplicación de reglas iterativamente para mejorar el etiquetado: Se cambiará una etiqueta de \textcolor{SchoolColor}{a} a \textcolor{SchoolColor}{b} cuando se cumpla alguna de las siguiente reglas de la lista de plantillas de reglas:
\begin{itemize}
\item La palabra siguiente(anterior) está etiquetada como \textcolor{SchoolColor}{z}
\item La segunda palabra previa( posterior) está etiquetada como \textcolor{SchoolColor}{z}
\item Una de las dos palabras anteriores(siguientes) está etiquetada como \textcolor{SchoolColor}{z}
\item Una de las tres palabras anteriores(siguientes) está etiquetada como \textcolor{SchoolColor}{z}
\item La siguiente está etiquetada como \textcolor{SchoolColor}{z} y la posterior como \textcolor{SchoolColor}{w}
\item La anterior(siguiente) está etiquetada como \textcolor{SchoolColor}{z} y las dos anteriores(posteriores) a ella están etiquetadas como \textcolor{SchoolColor}{w}
\item La palabra empieza (o no empieza) por mayúscula
\item La palabra anterior empieza (o no empieza) por mayúscula
\end{itemize} 
\end{enumerate}
Donde \textcolor{SchoolColor}{z}, \textcolor{SchoolColor}{w} son dos categorías cualesquiera (VB,NN..).
Cabe destacar que las reglas son aplicables tanto a palabras como a etiquetas de las mismas, y se expresan con la notación interna del algoritmo:\newline \texttt{"PREVTAG", "NEXTTAG", "PREV1OR2TAG", "NEXT1OR2TAG", "PREV1OR2OR3TAG", "NEXT1OR2OR3TAG", PREV2TAG","NEXT2TAG",
 "CURWD", "PREVWD", "NEXTWD", "PREV1OR2WD", "NEXT1OR2WD"}.\newline Un ejemlo de instancia de una regla sería: \textcolor{SchoolColor}{\texttt{TO IN NEXTTAG AT}}, que se interpretaría como que la etiqueta \texttt{"TO"} cambiaría a \texttt{"IN"} si la etiqueta de la palabra anterior es \texttt{"AT"}. \\[\baselineskip]
 Tras la fase del etiquetado inicial y etiquetar las desnococidas, se comparan las etiquetas estimadas con las reales, obtieniendo una lista de errores. A continuación, por cada error de esa lista (por cada palabra mal etiquetada) se instancia cada una de las reglas de la lista de plantillas de reglas, y se evalúan, para determinar cuál de ellas reduce más el error. La ganadora se aplica al texto y se añade a la lista de resultados. Después se vuelve a calcular una lista de errores sobre el nuevo corpus resultante de haber aplicado la mejor regla. \newline
 Para determinar cual es la mejor regla, a cada regla se le asigna una puntuación, que se calcula como   
 los aciertos que tiene menos los fallos.    


\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Procedure{BrillTagger(corpus,lexicon,rate\_error,rulesTlist)}{}
    \State $\text{$C_{i}$} \gets \text{initial tagging of corpus}$
\State $\text{errors} \gets \text{compute errors of $C_{i}$}$
\State $\text{Result} \gets$ \o
\State $\text{i} \gets 0$
\State $\text{nErrors} \gets \text{number of errors}$
\State $\text{best}  \gets$ \o
\While{$\text{nErrors} $>$ \text{rate\_error}$} 
        \For{$\text{each error in errors}$ }
            \For{$\text{each rule in instances(rulesTlist) }$}
            \State $\text{score(rule)} \gets \text{good corrections of rule - bad corrections of rule}$ \text{ in $C_{i}$}
            \EndFor
        \EndFor
        \State $\text{best} \gets \text{rule with best score}$
		\State $\text{$C_{i+1}$} \gets \text{Apply(best, $C_{i}$ )}$
		\State $\text{Results} \gets \text{Results+best}$
		\State $\text{i} \gets \text{i+1}$
        \EndWhile
        \EndProcedure
        \BState \Return Results
    \end{algorithmic}
    \label{alg:rAP}
    \caption{Algoritmo de Brill}
\end{algorithm}

\subsection{Lectura del corpus}
El Wikicorpus para español contiene unos 50 ficheros de texto. Cada uno pesa entre 25MB y 102MB y cada línea de cada uno de ellos es una palabra con su lema y su POS etiqueta en notación \textcolor{SchoolColor}{Parole} \url{http://www.cs.upc.edu/~nlp/SVMTool/parole.html}. 
Un fragmento de ejemplo:
\begin{minted}
[frame=lines,
framesep=1mm,
fontsize=\footnotesize,
breaklines=true
]{java}
 Ibiševi ibiševi NP00000 0
, , Fc 0
nacido nacer VMP00SM 00249716
el el DA0MS0 0
6_de_agosto_de_1984 [??:6/8/1984:??.??:??] W 0
en en SPS00 0
Vlasenica vlasenica NP00000 0
, , Fc 0
Yugoslavia yugoslavia NP00000 0
( ( Fpa 0
actual actual AQ0CS0 01667781
Bosnia bosnia NP00000 0
y y CC 0
Herzegovina herzegovina NP00000 0
) ) Fpt 0
es ser VSIP3S0 01775973
un uno DI0MS0 0
futbolista futbolista NCCS000 0
bosnio bosnio AQ0MS0 02731920
que que PR0CN000 0
juega jugar VMIP3S0 00727813
como como CS 0
delantero delantero NCMS000 00466114
\end{minted}

Aunque en este ejemplo no se aprecia, algunas líneas de estos ficheros son metadatos("<doc>", "</doc>", "ENDOFARTICLE", "REDIRECT"...), por lo que deben ser ignoradas. A continuación se muestra un pseudocódigo de la lectura de este corpus, obteniendo finalmente una lista de pares (palabra, etiqueta). El código en python que emplea \textcolor{SchoolColor}{Pattern} se puede encontrar en la web de \textcolor{SchoolColor}{CLiPS} (\url{http://www.clips.ua.ac.be/}). 

\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Procedure{readWikicorpus(corpusPath="./tagged.es",words=1000000}{}
    \State $\text{S} \gets$ \o
     \State $ \text{i} \gets 0$
    \For{$\text{f in corpusPath}$}
            \For{$\text{line in open(f)}$}
            \If{\text{line $==$ " \escape{n} " or startsWith(metadata)}} \State $\text{continue}$ \EndIf
            \State $\text{w, lemma, tag} \gets \text{line splitted by (" ")}$
            \If{$\text{tag starts with("Fp")}$ }  $\text{tag=2 first characters of tag}$  
            \ElsIf{$\text{tag starts with("V")}$} $\text{tag=3 first characters of tag}$
            \ElsIf{$\text{tag starts with("NC")}$}  $\text{tag=2 first characters of tag + $3^{\circ}$ character of tag}$
             \Else $\text{tag=2 first characters of tag}$ \EndIf
            \EndFor
            \For{$\text{w in split tag by ("\_")}$} 
            \State $\text{S} \gets \text{(w,tag)}$
             \State $ \text{i} \gets \text{i+1}$
            \EndFor
            \If{\text{tag $==$ "Fp" and w $==$ "."} } \text{$S \gets [ ]$} \EndIf
            \If{$i \geq \text{words}$} \Return S \EndIf
        \EndFor
        \EndProcedure
    \end{algorithmic}
    \label{alg:rAP}
    \caption{Lectura Wikicorpus}
\end{algorithm}
\subsection{Obtención del Léxico}
El método de Pos tagger español propuesto en \citet{smedt2012pattern}, y por lo tanto, el implementado en este proyecto, hace uso de reglas léxicas, morfológicas y contextuales para mejorar la calidad del etiquetado. \textcolor{SchoolColor}{Pattern} usa las herramientas de \textcolor{SchoolColor}{PythonNLTK} para obtener estos ficheros de reglas. En esta sección se explica como se construye el diccionario léxico usado en este trabajo a partir de el corpus leído en la sección anterior. 
El procedimiento a seguir es el siguiente:
\begin{enumerate}
\item Se cuenta la ocurrencia de cada palabra en el Wikicorpus español
\item Se cuenta la ocurrencia de las etiquetas de las palabras en el mismo corpus. Hay palabras que pueden tener varias etiquetas
\item Se construye el diccionario tomando las palabras más frecuentes y su etiqueta más frecuente
\end{enumerate}
Este diccionario léxico se utilizará luego para etiquetar inicialmente los datos en el algoritmo de Brill. De nuevo, se expone una descripción en pseudocódigo de este proceso:
\begin{algorithm}[H]
    \begin{algorithmic}[1]
    \Procedure{createLexicon(corpus, maxLines=100000)}{}
    \State $\text{Lexicon} \gets$ \o
    \State $\text{top} \gets$ \o
    \For{$\text{sentence in corpus}$}
            \For{$\text{w, tag in sentence}$}
            \State $\text{Lexicon(w, tag)} \gets \text{+=1}$
            \EndFor
    \EndFor        
    \For{$\text{w, tags in Lexicon}$} 
            \State $\text{freq} \gets \text{$\sum$ all ocurrences of tags for w}$
             \State $\text{tag} \gets \text{the tag with bigger ocurrence for w}$
             \State $\text{top} \gets \text{top + (freq,w,tag) }$
            \EndFor
            
     \State $\text{get the maxLines with bigger ocurrency}$      
        \EndProcedure
    \end{algorithmic}
    \label{alg:rAP}
    \caption{Obtención del Léxico}
\end{algorithm}
Debido a que son ficheros muy grandes y entrenar los algoritmos para obtener el léxico, el contexto y la morfología cada vez que se quiera usar el Pos Tagger es un proceso muy lento y poco recomendable, los algoritmos se ejecutan una vez, guardando los resultados en ficheros para reutilizarlos posteriormente. El léxico se guarda en el fichero  \textcolor{SchoolColor}{es-lexicon.txt}, y se usa directamente en el Pos tagger que implementa este proyecto.

\begin{minted}
[frame=lines,
framesep=1mm,
fontsize=\footnotesize,
breaklines=true
]{java}
óleos NCP
ómnibus NC
ónice NCS
ópera NCS
óperas NCP
óptica AQ
ópticas AQ
\end{minted}
\captionof{listing}*{Líneas de ejemplo de es-lexicon.txt}

