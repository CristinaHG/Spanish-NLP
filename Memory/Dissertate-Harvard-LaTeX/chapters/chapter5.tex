%!TEX root = ../dissertation.tex
\chapter{Implementación y pruebas}
\label{conclusion}

En este capítulo se explican la implementaciones llevadas a cabo en este trabajo y las pruebas que se han realizado sobre las mismas. Los algoritmos se describen por orden en el pipeline. 
\section{Tokenizador}
El proceso de tokenización implementado recibe como entrada una cadena de texto y da como salida los tokens de la cadena. Si la cadena se compone de varias frases, entonces da como salida el texto segmentado en frases tokenizado. Por ejemplo, si recibe como entrada: \newline
\begin{center}\texttt{"El cielo es azul. El agua es transparente y las amapolas son rojas."}
\end{center}
Dará como salida:
 \texttt{["El", "cielo", "es", "azul", "."]} \newline
\texttt{["El", "agua", "es", "transparente", "y", "las", "amapolas", "son", "rojas", "."]}\newline
El algoritmo de tokenizado se puede dividir en 4 fases:
\begin{enumerate}
\item Preprocesado el string: manejo de comillas (simples y dobles), espacios en blanco, retornos de carro, saltos de línea.
\item Obtención de los tokens del texto
\item Separación de frases
%\item Manejo de sarcasmo y emoticonos
\end{enumerate}

Tanto el algoritmo como las estructuras usadas por éste se implementan en una clase \textcolor{SchoolColor}{Tokenizer}. Igual se ha hecho con el resto de clases, pretendiendo así aislar todo la funcionalidad relativa a una clase en la implementación de la misma. Veamos el algoritmo en detalle:
\subsection{Preprocesado del texto recibido}
\begin{minted}
[frame=lines,
framesep=1mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def find_tokens(string:String):List[List[String]]={
    var s=string
    val punc=punctuation.replace(".","").toCharArray

  //handle unicode quotes
    if(s.contains("“")) s.replace("“"," “ ")
    if(s.contains("”")) s.replace("”"," ” ")
    if(s.contains("‘")) s.replace("‘"," ‘ ")
    if(s.contains("’")) s.replace("’"," ’ ")
   //collapse whitespace
    s="""\r\n""".r.replaceAllIn(s,"\n")
    s="""\n{2,}""".r.replaceAllIn(s,EOS)
    s="""\s+""".r.replaceAllIn(s," ")
\end{minted}
Donde \textsf{punctuation} es una cadena con signos de puntuación, que se define dentro de la clase como: 
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
  private[this] val punctuation = ".,;:!?()[]{}`'\"@#\$^&*+-|=~_"
\end{minted}
En la segunda línea de la función se elimina el punto de entre los signos de puntuación, dado que el punto lo manejaremos aparte, para evitar confusiones como creer que \textsf{"..."} es un final de frase al leer su primer punto. Con eso se construye un array de caracteres \textsf{punc}. \newline
El siguiente paso es espaciar las comillas del texto, ya que el tokenizador busca tokens entre espacios en blanco. Por ejemplo, la frase:  \texttt{Como dice la canción,"vive y sé feliz".} Quedaría:  
\texttt{Como dice la canción, " vive y sé feliz " .} \newline
Finalmente se hace uso de expresiones regulares para sustituir la presencia de \textsf{\escape{r}\escape{n}}, que es el separador de línea que suele usar Windows por un salto de línea, así como cambiar uno o más caracteres en blanco por uno solo y dos o más saltos de línea por \textsf{EOS}, definida anteriormente dentro de la clase como:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
  private[this] val EOS = "END-OF-SENTENCE"
\end{minted}

\subsection{Obtención de los tokens}
Para obtener los tokens se define el token con una expresión regular de la siguiente forma:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
private[this] var TOKEN="""(\S+)\s""".r
\end{minted}
Es decir, uno o más caracteres que no sean espacios en blanco seguidos de un espacio en blanco. Esta expresión regular se usa para encontrar todas las coincidencias de palabras seguidas de un espacio en blanco en el texto, en este caso \textsf{s}. Luego, se itera sobre cada una de las coincidencias encontradas de la siguiente manera: \newline
Se comienza eliminando el espacio en blanco final de la palabra: por ejemplo \textsf{"gatos "} $\Rightarrow$ \textsf{"gatos"}, y se comprueba si la primera letra de la palabra es un signo de puntuación distinto del punto. Si lo es, se quita la primera letra de la palabra y se mete en la lista de tokens, repitiendo el mecanismo hasta que la primera letra de la palabra no sea un signo de puntuación. Después comienza de nuevo un proceso iterativo, en el que se comprueba si la última letra de la palabra es un signo de puntuación, incluido el punto. Aquí pueden darse tres casos:
\begin{itemize}
\item Si el final de la palabra son puntos suspensivos: Por ejemplo \textsf{"cantaba..."} en este caso  se quitan los tres puntos finales y se añaden a una lista para ser añadidos después.
\item Si el final de la palabra es un signo de puntuación pero no es ningún punto: se quita el signo de puntuación y se añade a una la lista, para ser añadido después.
\item Si la palabra acaba con un punto: en ese caso se comprueba si es una abreviación (Por ejemplo: Srta.) para ello se hace uso de una lista de abreviaciones y expresiones regulares definidas en la clase: 
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
 private[this] val abbreviations=List("a.C.", "a.m.", "apdo.", "aprox.", "Av.", "Avda.", "c.c.", "D.", "Da.", "d.C.",
    "d.j.C.", "dna.", "Dr.", "Dra.", "esq.", "etc.", "Gob.", "h.", "m.n.", "no.",
    "núm.", "pág.", "P.D.", "P.S.", "p.ej.", "p.m.", "Profa.", "q.e.p.d.", "S.A.",
    "S.L.", "Sr.", "Sra.", "Srta.", "s.s.s.", "tel.", "Ud.", "Vd.", "Uds.", "Vds.",
    "v.", "vol.", "W.C.")

private[this] val re_abbr1="""^([A-Za-z]\.)+\$""".r 
private[this] val re_abbr2="""^[A-Z][a-z]{1,3}\.\$""".r 
\end{minted}
donde \textcolor{SchoolColor}{re\_abbr1} empareja abreviaciones como \textsf{D.,U.S.,c.c.}, 
y \textcolor{SchoolColor}{re\_abbr2} empareja abreviaciones como \textsf{Dr.,Sr.,Dra.,Uds.}.
Si se reconoce como abreviación, se mete en la lista de tokens, mientras que si no es una abreviación se quita el punto y se añade a una la lista, para ser añadido después.
\item Finalmente se añaden tanto la palabra como el signo de puntuación que se ha ido guardando para después en la lista de tokens. La descripción en código es la siguiente:
\end{itemize}   

\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
var tokens=List[String]()

  //get tokens and handle punctuation marks
  TOKEN.findAllIn(s+" ").foreach(t => if(t.length>0){
   var tail=mutable.MutableList[String]()
   var t2=t.stripSuffix(" ")
   while (punc.contains(t2.head)){
    tokens::=t2.head.toString
    //tokens=tokens.reverse
    t2=t2.tail
   }
   breakable {
    while (punc.contains(t2.last) || t2.endsWith(".")) {
     if (t2.endsWith("...")) {
      tail += ("...")
      t2 = t2.substring(0, t2.length - 3)
     }
     else if (punc.contains(t2.last)) {
      tail += (t2.substring(t2.length - 1))
      t2 = t2.substring(0, t2.length - 1)
     } //split elipsis (...) before splitting period

     //split period(if not an abbreviation)
     if (t2.endsWith(".")) {
      if ((abbreviations.contains(t2) || re_abbr1.findAllMatchIn(t2).length > 0 || re_abbr2.findAllMatchIn(t2).length > 0) != true) {
       tail += (t2.substring(t2.length - 1))
       t2 = t2.substring(0, t2.length - 1)
      }else break()
     }
    }
   }

   if( t2.compareTo("")!=0){
    tokens::=t2
   }
   if(!tail.isEmpty) {
    tail.foreach(u=> tokens::=u.toString )
   }
  })
\end{minted}
 Puesto que en el código se ha ido insertando por la cola, sólo quedaría invertir la lista para tener los tokens:
 \begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
 tokens=tokens.reverse
\end{minted}
\subsection{Separación de frases} 
Obtenida la lista de tokens, queda segmentarla por frases. El procedimiento es el siguiente: se \newline itera sobre la lista de tokens obtenida, comprobando si el token es algún signo de fin de sentencia \textsf{("...",".","!","?","EOS")}. Cuando se encuentra un signo de fin de sentencia, se toman todos los tokens \newline leídos (incluido el de final de sentencia) como una frase. A partir de aquí empieza a considerarse que empieza la frase siguiente. El proceso termina cuando se ha iterado sobre todos los tokens. Su descripción en código, es:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
  var j=0
  var i=0
  var sentences=List[String]()
   while (j < tokens.length) {

    if (tokens(j) == "..." || tokens(j) == "." || tokens(j) == "!" || tokens(j) == "?" || tokens(j) == EOS) {
     j += 1
     sentences ::= tokens.slice(i, j).filter(t => t != EOS).mkString(" ")
     i = j
    }
    j += 1
   }
  sentences::=tokens.slice(i,j).filter(t=>t!=EOS).mkString(" ")
\end{minted}
Hay frases que no tienen signo de puntuación final. Se podría considerar que si no tiene signo de puntuación final no está bien estructurada y por lo tanto no es válida, pero se ha preferido considerar que si no tiene signo de puntuación final, la frase se acaba cuando no hay más tokens. Por ejemplo \texttt{"Ese boli negro escribe muy bien"} se entendería como una única frase.
\\[\baselineskip]
Debido a esto, se añade la última línea de código anterior, ya fuera del bucle. Para textos bien estructurados (con frases con una marca de fin) esta sentencia última resulta en añadir un sentencia vacía al conjunto de sentencias, sin embargo esta se filtra antes de devolver el resultado de forma muy cómoda en \texttt{Scala}. \newline   
%\subsection{Manejo de sarcasmo y emoticonos} 
%Finalmente, se aplican las expresiones regulares para señalizar una posible muestra de  sarcasmo y el manejo de %emoticonos y se devuelve el resultado como lostas de listas de cadenas, es decir, como una lista de frases.
%\begin{minted}
%[frame=lines,
%framesep=1.3mm,
%fontsize=\footnotesize,
%breaklines=true
%]{scala}
 %sentences=sentences.map(s=>re_sarcasm.replaceAllIn(s,"(!)"))
%  sentences=sentences.map(s=> RE_EMOTICONS.replaceAllIn(s, m=> s"\${m.group(1).replace(" ","")+m.group(2)}"))
%  return sentences.reverse.filter(s=> !s.isEmpty).map(t => t.split(" ").toList)
%\end{minted}
%donde \textcolor{SchoolColor}{re\_sarcasm} se define arriba como:
%\begin{minted}
%[frame=lines,
%framesep=1.3mm,
%fontsize=\footnotesize,
%breaklines=true
%]{scala}
%private[this] val re_sarcasm="""\( ?\! ?\)""".r
%\end{minted}
%y \textcolor{SchoolColor}{RE\_EMOTICONS} como:
%\begin{minted}
%[frame=lines,
%framesep=1.3mm,
%fontsize=\footnotesize,
%breaklines=true
%]{scala}
%private[this] val emoticons= mutable.Map[(String,Double),List[String]]()
%  emoticons+=(("love" , 1.00) ->List("<3","♥"))
%  emoticons+=(("grin" , 1.00)->List(">:D", ":-D", ":D", "=-D", "=D", "X-D", "x-D", "XD", "xD", "8-D"))
%  emoticons+=(("taunt", +0.75)->List(">:P", ":-P", ":P", ":-p", ":p", ":-b", ":b", ":c)", ":o)", ":^)"))
%  emoticons+=(("smile", +0.50)->List(">:)", ":-)", ":)", "=)", "=]", ":]", ":}", ":>", ":3", "8)", "8-)"))
%  emoticons+=(("wink" , +0.25)->List(">;]", ";-)", ";)", ";-]", ";]", ";D", ";^)", "*-)", "*)"))
%  emoticons+=(("gasp" , +0.05)->List(">:o", ":-O", ":O", ":o", ":-o", "o_O", "o.O", "°O°", "°o°"))
%  emoticons+=(("worry", -0.25)->List(">:/",  ":-/", ":/", ":\\", ">:\\", ":-.", ":-s", ":s", ":S", ":-S", ">.>"))
%  emoticons+=(("frown", -0.75)->List(">:[", ":-(", ":(", "=(", ":-[", ":[", ":{", ":-<", ":c", ":-c", "=/"))
 % emoticons+=(("cry"  , -1.00)->List(":'(", ":'''(", ";'("))

  %private[this] var re_emoticons=""::Nil
  %//separating emojis by "|"
  %emoticons.values.foreach(list=>re_emoticons:::=list.flatMap(elem=> if(elem.compareTo(list.last)!=0) "|"::elem::Nil else "|"::elem::"|"::Nil).tail)
  %private[this] var re1_emoticons=""::Nil
  %//scaping each char in icons
  %re_emoticons.foreach(icon=>if(!(icon.equals("|"))) re1_emoticons::=icon.mkString("?".concat("""\""")) else re1_emoticons::=icon )
%  // if letters= "D" "S" "s" "b" or "c" do not scape
 % re1_emoticons=re1_emoticons.map(t=>
  %    if( t.contains("D")) t.dropRight(t.length-t.indexOf("D")+1)+t.last
   %   else if (t.contains("S")) t.dropRight(t.length-t.indexOf("S")+1)+t.last
  %    else if (t.contains("s")) t.dropRight(t.length-t.indexOf("s")+1)+t.last
    %  else if (t.contains("b")) t.dropRight(t.length-t.indexOf("b")+1)+t.last
     % else if (t.contains("c")) t.dropRight(t.length-t.indexOf("c")+1)+t.last
      %else t
  %)
  %// scape first char in emoji
  %re1_emoticons=re1_emoticons.reverse.tail.map(t => if(!(t.equals("|"))) """\"""+t else t )  
  %//create emoji regex
  %private[this] var RE_EMOTICONS=Pattern.quote(re1_emoticons.dropRight(2).mkString).r
%\end{minted}

Por último, en la clase tokenizador también se implementan dos métodos que imprimen las sentencias de tokens y cuentan las sentencias de tokens, respectivamente. 
%\begin{minted}
%[frame=lines,
%framesep=1.3mm,
%fontsize=\footnotesize,
%breaklines=true
%]{scala}
%def get_sentences(sentences:List[String]): Unit ={
 % sentences.foreach(p=>print(p+"\n"))
 %}

 %def count_sentences(sentences:List[List[String]]): Int ={
 % return sentences.length
% }
%\end{minted} 
\section{POS Tagger}
El algoritmo de Pos tagger realiza el etiquetado morfosintáctico del texto. Para ello recibe una lista de tokens y devuelve una lista de tuplas (token, etiqueta) de los mismos. Para el etiquetado se usan etiquetas en el formato de \textsc{Parole} \url{http://www.lsi.upc.edu/~nlp/SVMTool/parole.html}  pero se añade opción de especificar una función que las mapee a otro formato como PennTreebank o el formato universal. \newline
Los parámetros que recibe el algoritmo son: 
\begin{itemize}
\item una lista de tokens
\item un objeto de la clase léxico
\item un objeto de la clase morfología
\item un objeto de la clase contexto
\item una lista de string con tres etiquetas por defecto("NCS","NP","Z")
\item una función para mapear el formato de las etiquetas a otro formato (opcional)
\end{itemize}

El funcionamiento es el siguiente: 
\begin{enumerate}
\item Etiquetado inicial: se itera sobre los tokens, etiquetando las palabras conocidas. Para ello se buscan en el diccionario de léxico que se especifica por parámetro, asignándole la etiqueta que dicha palabra tenga el diccionario léxico. Si no se encuentra la palabra en el diccionario, esta se etiqueta como \texttt{"None"} en esta primera fase del algoritmo.
\item Etiquetar las desconocidas: para ello se itera sobre la colección obtenida en el paso anterior. Si una palabra tiene etiqueta desconocida pero es una palabra que empieza por mayúscula, se cambia su etiqueta a \texttt{"NP"} (nombre propio). Si por el contrario, casa con la expresión regular:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
 private[this] val CD = """^[0-9\-\,\.\:\/\%\$]+\$"""
 \end{minted}
 es decir, es un dígito, símbolo o combinación de ambos, se etiqueta como \texttt{"Z"} (dígito). Si se ha proporcionado una morfología al algoritmo, se aplican las reglas morfológicas presentes en la misma para cambiar la etiqueta de la palabra. Por último, si ninguna de las anteriores aplica, se etiqueta como \texttt{"NCS"}.
 \item Aplicación de reglas contextuales: Si se ha especificado un contexto, éste se aplica para mejorar el etiquetado anterior de todas las palabras.
 \item Si se ha especificado una función para mapear las etiquetas a otro formato, ésta se aplica sobre el resultado anterior.   
\end{enumerate} 
El algoritmo en código se encuentra dentro de la clase \textcolor{SchoolColor}{PosTagger} y es el siguiente:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def find_tags(tokens:List[String], lexicon:Lexicon, morphology:Morphology, context:Context, default:List[String],
                mapCall:(String,String)=>(String,String)):List[(String,String)]={

    var tagged=List[(String,String)]()
    var taggedMorp=List[(String,String)]()
    var taggedCntxt=List[(String,String)]()
    var taggedfin=List[(String,String)]()
    // Tag known words.
    tokens.foreach(t=> tagged::=(t,lexicon.getLexDict.getOrElse(t,lexicon.getLexDict.getOrElse(t.toLowerCase,"None"))))
    //Tag unknow words
    tagged=tagged.reverse
    taggedMorp=tagged.map(t=> {
      var prev = ("None", "None")
      var next = ("None", "None")
      if (tagged.indexOf(t) > 0) prev = tagged(tagged.indexOf(t) - 1)
      if (tagged.indexOf(t) < (tagged.length - 1)) next = tagged(tagged.indexOf(t) + 1)
      if (t._2 == "None") {
        //use NP for capitalized words
        if (t._1.matches("""^[A-Z][a-z]+.\$""")) (t._1, default(1))
        //use CD for digits and numbers
        else if (t._1.matches(CD)) (t._1, default(2))
        //use suffix rules (ej, -mente=ADV)
        else if (!morphology.getMorphology.isEmpty) (t._1,morphology.apply(t._1, default(0), prev, next, lexicon.getLexDict))
          // Use most frequent tag (NCS).
        else (t._1, default(0))

      } else (t._1,t._2)
    })
    //Tag words by context
    if(!context.getContextList.isEmpty ) taggedCntxt=context.apply(taggedMorp)
    else taggedCntxt=taggedMorp
    //Map tag with a custom function
    if(mapCall != null){
      taggedCntxt.foreach(t => taggedfin ::= mapCall(t._1, t._2))
      taggedfin=taggedfin.reverse
    } else taggedfin=taggedCntxt
    return taggedfin
  }
\end{minted}

\subsection{Aplicación del léxico}
En la clase \textcolor{SchoolColor}{Lexicon} se implementa un método que lee el fichero \textcolor{SchoolColor}{es-lexicon.txt} y crea un Diccionario Léxico como dato miembro de la clase. La cabecera del método de lectura es la siguiente:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
 def read(path: String, encoding: String, comment: String): Unit
\end{minted}
recibiendo como parámetros la ruta del fichero, la codificación y el formato de los comentarios del mismo. La aplicación del léxico en el método anterior es trivial, pues no es más que consultar un diccionario.
\subsection{Aplicación de la morfología}
En la clase \textcolor{SchoolColor}{Morphology} se implementa un método que lee el fichero \textcolor{SchoolColor}{es-morphology.txt} y crea una lista interna a la clase de reglas morfológicas (\textsf{morphologyList}). La cabecera es igual que la descrita justo arriba. En esta clase se definen los comandos de las reglas morfológicas, teniendo en cuenta los prefijos-sufijos de las palabras, como: 
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
private [this] var rulesSet= Set(
    "word", // Word is x.
    "char", // Word contains x.
    "haspref", // Word starts with x.
    "hassuf", // Word end with x.
    "addpref", // x + word is in lexicon.
    "addsuf", // Word + x is in lexicon.
    "deletepref", // Word without x at the start is in lexicon.
    "deletesuf", // Word without x at the end is in lexicon.
    "goodleft", // Word preceded by word x.
    "goodright"// Word followed by word x.
  )
   rulesSet.foreach(r=> rulesSet.+=("f"+r.mkString))
\end{minted}
Estas reglas se emplean en el método \textcolor{SchoolColor}{apply} de esta clase, que aplica la morfología a un par \newline (palabra,etiqueta), dado el par previo a la misma de (palabra,etiqueta), y el posterior. \newline
El proceso es éste: se itera sobre la lista de reglas extraídas del fichero (\textsf{morphologyList}), y se comprueba si el comando de la regla está en \textsf{rulesSet}. Las reglas son del tipo:  \texttt{ly hassuf 2 RB x} o  \texttt{NN s fhassuf 1 NNS x}, donde el comando es \textsf{"hassuf y fhassuf"} respectivamente. Luego se aplican las reglas de \textsf{rulesSet} a la palabra que se quiere etiquetar y si se cumple alguna, se cambia la etiqueta de la palabra a la que indique la regla actual de \textsf{morphologyList}.
El proceso en código es:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
 def apply(token:String,tag:String,previus:(String,String), next:(String,String),lexicon:mutable.Map[String,String]): String ={

    var f = false
    var x =""
    var cmd=""
    var pos=""
    var realTag=""

    morphologyList.foreach(l=> {
      if (rulesSet.contains(l(1))) { // Rule = ly hassuf 2 RB x
        f = false
        x = l(0)
        pos = l(l.length - 2)
        cmd = l(1).toLowerCase
      }
      if (rulesSet.contains(l(2))) { // Rule = NN s fhassuf 1 NNS x
         f = true
         x = l(1)
         pos= l(l.length -2)
         cmd= l(2).toLowerCase.stripPrefix("f")
      }
      if( f==false || tag.compareTo(l(0))==true){
        if((cmd=="word" && x==token) ||
          (cmd=="char" && token.contains(x)) ||
          (cmd=="haspref" && token.startsWith(x)) ||
          (cmd=="hassuf" && token.endsWith(x)) ||
          (cmd=="addpref" && lexicon.contains(x+token)) ||
          (cmd=="addsuf" && lexicon.contains(token+x)) ||
          (cmd=="deletepref" && token.startsWith(x) && lexicon.contains(token.substring(x.length))) ||
          (cmd=="deletesuf" && token.endsWith(x) && lexicon.contains(token.substring(0,token.length-x.length))) ||
          (cmd=="goodleft" && x==next._1) ||
          (cmd=="goodright" && x==previus._1)
        ){
          realTag=pos
        }
      } else realTag=tag
    })
    return realTag
  }
\end{minted}

\subsection{Aplicación del contexto}
En la clase \textcolor{SchoolColor}{Context} se implementa un método que lee el fichero \textcolor{SchoolColor}{es-context.txt} creando una lista de reglas contextuales en la clase (\texttt{ContextList}). Recordemos que el algorito de brill creaba reglas de la forma: \texttt{VBD VB PREVTAG IN => una palabra etiquetada como VBD cambia a  VB si la etiqueta de la anterior a ella es IN.}. Por tanto, en la clase se definen las reglas basadas en contexto como:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
private[this] var rulesSet= Set(
    "prevtag", // Preceding word is tagged x.
    "nexttag", // Following word is tagged x.
    "prev2tag", // Word 2 before is tagged x.
    "next2tag", // Word 2 after is tagged x.
    "prev1or2tag", // One of 2 preceding words is tagged x.
    "next1or2tag", // One of 2 following words is tagged x.
    "prev1or2or3tag", // One of 3 preceding words is tagged x.
    "next1or2or3tag", // One of 3 following words is tagged x.
    "surroundtag", // Preceding word is tagged x and following word is tagged y.
    "curwd", // Current word is x.
    "prevwd", // Preceding word is x.
    "nextwd", // Following word is x.
    "prev1or2wd", // One of 2 preceding words is x.
    "next1or2wd", // One of 2 following words is x.
    "next1or2or3wd", // One of 3 preceding words is x.
    "prev1or2or3wd", // One of 3 following words is x.
    "prevwdtag", // Preceding word is x and tagged y.
    "nextwdtag", // Following word is x and tagged y.
    "wdprevtag", // Current word is y and preceding word is tagged x.
    "wdnexttag", // Current word is x and following word is tagged y.
    "wdand2aft", // Current word is x and word 2 after is y.
    "wdand2tagbfr", // Current word is y and word 2 before is tagged x.
    "wdand2tagaft", // Current word is x and word 2 after is tagged y.
    "lbigram", // Current word is y and word before is x.
    "rbigram", // Current word is x and word after is y.
    "prevbigram", // Preceding word is tagged x and word before is tagged y.
    "nextbigram" // Following word is tagged x and word after is tagged y.
  )
\end{minted}
El procedimiento de aplicación de las reglas contextuales es similar al de las morfológicas: \newline
se tiene una lista de tuplas de (token, etiqueta) a la que se le añaden tres delimitadores por cada extremo y se itera sobre ella.\newline En cada elemento de esa lista se itera sobre cada regla de \texttt{contextList} , y si la etiqueta de la tupla (token, etiqueta) actual no es un delimitador y ésta coincide con la primera etiqueta de la regla de \texttt{contextList} sobre la que se itera, se comprueban las reglas contextuales correspondientes a la etiqueta considerada y el comando de la regla actual de \texttt{contextList}. Si empareja alguna, se modifica la etiqueta de la tupla con la etiqueta que indique la regla. Finalmente se eliminan los delimitadores del resultado.
El proceso en código es como sigue:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def apply(tokensTags:List[(String,String)]):List[(String,String)]={
    val o=List(("STAART", "STAART"),("STAART", "STAART"),("STAART", "STAART")) //empty delimiters for look ahead/back

   var t=o.++(tokensTags).++(o)
  // var mapped=mutable.MutableList[(String,String)]()
  var mapped=List[(String,String)]()
    var cmd=""
    var x=""
    var y=""
    var r1=""
    var matches=false
    var index=0

   t.foreach(token=> { this.contextList.foreach(r=>
     if ((token._2 != "STAART") && (token._2 == r(0) || r(0) == "*")) {
       cmd = r(2).toLowerCase
       x = r(3)
       y = if (r.length > 4) r(4) else ""

       if ((cmd == "prevtag" && x == t(index - 1)._2) ||
         (cmd == "nexttag" && x == t(index + 1)._2) ||
         (cmd == "prev2tag" && x == t(index - 2)._2) ||
         (cmd == "next2tag" && x == t(index + 2)._2) ||
         (cmd == "prev1or2tag" && ((t(index - 1)._2, t(index - 2)._2).toString().contains(x))) ||
         (cmd == "next1or2tag" && ((t(index + 1)._2, t(index + 2)._2).toString().contains(x))) ||
         (cmd == "prev1or2or3tag" && ((t(index - 1)._2, t(index - 2)._2, t(index - 3)._2).toString().contains(x))) ||
         (cmd == "next1or2or3tag" && ((t(index + 1)._2, t(index + 2)._2, t(index + 3)._2).toString().contains(x))) ||
         (cmd == "surroundtag" && (x == t(index - 1)._2 && y == t(index + 1)._2)) ||
         (cmd == "curwd" && (x == t(index)._1)) ||
         (cmd == "prevwd" && (x == t(index - 1)._1)) ||
         (cmd == "nextwd" && (x == t(index + 1)._1)) ||
         (cmd == "prev1or2wd" && (t(index - 1)._1, t(index - 2)._1).toString().contains(x)) ||
         (cmd == "next1or2wd" && (t(index + 1)._1, t(index + 2)._1).toString().contains(x)) ||
         (cmd == "prevwdtag" && (x == t(index - 1)._1 && y == t(index - 1)._2)) ||
         (cmd == "nextwdtag" && (x == t(index + 1)._1 && y == t(index + 1)._2)) ||
         (cmd == "wdprevtag" && (x == t(index - 1)._2 && y == t(index)._1)) ||
         (cmd == "wdnexttag" && (x == t(index)._1 && y == t(index + 1)._2)) ||
         (cmd == "wdand2aft" && (x == t(index)._1 && y == t(index + 2)._1)) ||
         (cmd == "wdand2tagbfr" && (x == t(index - 2)._2 && y == t(index)._1)) ||
         (cmd == "wdand2tagaft" && (x == t(index)._1 && y == t(index + 2)._2)) ||
         (cmd == "lbigram" && (x == t(index - 1)._1 && y == t(index)._1)) ||
         (cmd == "rbigram" && (x == t(index)._1 && y == t(index + 1)._1)) ||
         (cmd == "prevbigram" && (x == t(index - 2)._2 && y == t(index - 1)._2)) ||
         (cmd == "nextbigram" && (x == t(index + 1)._2 && y == t(index + 2)._2))
       ) {matches=true ;r1=r(1)}
     })
     if(matches){ mapped::=Tuple2(token._1,r1) ; matches=false} else mapped::=token
   index+=1
   })
    return mapped.reverse.filter(p=>p._1!="STAART")
  }
\end{minted}

\subsection{Funciones para mapear los tagsets}
Por último, mencionar que en la clase \textcolor{SchoolColor}{PosTagger} se implementan también algunas funciones que mapean las etiquetas de un tagset a otro, en concreto:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def parole2penntreebank(token:String,tag:String):(String, String)={
   return (token,parole.getOrElse(tag,tag))
  }
\end{minted}
mapea las etiquetas del tagset \textsc{parole} a \textsc{pennTreebank}

\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def penntreebank2universal(token: String, tag: String):(String,String)def penntreebank2universal(token: String, tag: String):(String,String) ={
    if (tag.startsWith("NNP-") || tag.startsWith("NNPS-")) return (token,(NOUN.concat(tag.split("-").toString.formatted("%s-%s"))))
      //return (token, "%s-%s" % (NOUN, tag.split("-")[-1]))
    if(List("NN", "NNS", "NNP", "NNPS", "NP").contains(tag)) return (token, NOUN)
    if(List("MD", "VB", "VBD", "VBG", "VBN", "VBP", "VBZ").contains(tag)) return (token, VERB)
    if(List("JJ", "JJR", "JJS").contains(tag)) return (token, ADJ)
    if(List("RB", "RBR", "RBS", "WRB").contains(tag)) return (token, ADV)
    if(List("PRP", "PRP\$", "WP", "WP\$").contains(tag)) return (token, PRON)
    if(List("DT", "PDT", "WDT", "EX").contains(tag)) return (token, DET)
    if(List("IN").contains(tag)) return (token, PREP)
    if(List("CD").contains(tag)) return (token, NUM)
    if(List("CC").contains(tag)) return (token, CONJ)
    if(List("UH").contains(tag)) return (token, INTJ)
    if(List("POS", "RP", "TO").contains(tag)) return (token, PRT)
    if(List("SYM", "LS", ".", "!", "?", ",", ":", "(", ")", "\"", "#", "\$").contains(tag)) return (token, PUNC)
    return (token, X)
  }
\end{minted}
 mapea las etiquetas de \textsc{pennTreebank} al  \textsc{universal} 
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def parole2universal(token:String, tag:String):(String,String)= {

    if(tag == "CS") return (token, CONJ)
    if(tag == "DP") return (token, DET)
    if(List("P0", "PD", "PI", "PP", "PR", "PT", "PX").contains(tag)) return (token, PRON)

    var paroletreebank=parole2penntreebank(token, tag)
    return penntreebank2universal(paroletreebank._1,paroletreebank._2)
  }
\end{minted}
mapea de \textsc{parole} al  \textsc{universal}.

\section{Lematizador}
Aunque en algunos softwares de PLN la lematización se hace previamente al etiquetado, en este trabajo se realiza después, ya que se hace uso de las etiquetas de los tokens en el algoritmo de lematización. \newline
En la clase \textcolor{SchoolColor}{Lematizer} se tiene un diccionario de lemas como dato miembro, que se construye a través del fichero \textcolor{SchoolColor}{es-verbs.txt}.
 \begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
private[this] var mappedVerbs = Map[String, String]()
  }
\end{minted}

El mecanismo de lematización en general es el siguiente: se recibe una lista de tuplas (token, etiqueta) y se itera sobre sus elementos. Entonces si la etiqueta es \texttt{"DT"(determinante)} o \texttt{"NNS"(nombre común en plural)} se llama al método \textcolor{SchoolColor}{singularize}, que pasa la palabra a su forma en singular. Si la etiqueta es \texttt{"JJ"(adjetivo)} se llama a \textcolor{SchoolColor}{predicative}, que se encarga de buscar la forma base de los adjetivos y si la etiqueta es \texttt{"VB" o "MD" (verbo o modal)} se llama a \textcolor{SchoolColor}{get\_lemma}, que busca la forma base de los verbos. En código:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def get_lemmas(l: List[(String,String)]): List[(String, String,String)] = {
    var word :String=""
    var lemma:String=""
    var pos:String=""
    var lemmalist: List[(String, String, String)] = List()

  l.foreach( i => {word=i._1;pos=i._2;lemma=i._1;
  if (pos.startsWith("DT")) lemma=singularize(word,"DT")
  if (pos.startsWith("JJ")) lemma=predicative(word)
  if (pos.startsWith("NNS")) lemma=singularize(word,"NNS")
    if (pos.startsWith("VB") || pos.startsWith("MD")) lemma= verb_lemma(word)
    lemmalist= (word,pos,lemma)::lemmalist
 })
    return lemmalist.reverse
  }
  }
\end{minted}
Veamos a continuación cada una de las funciones con más detalle.
\subsection{Singularize}
Pasa nombres y determinantes a su forma singular. Para ello comprueba si es un determinante. Si lo es y es \texttt{"la","las" o "los"} lo pasa a \texttt{"el"}. Análogamente con \texttt{"una","unas" o "unos"} los pasa a \texttt{"un"}.\newline
Luego se comprueba si la palabra acaba en \texttt{"es"(suele ser nombre plural)} y si la subcadena de la palabra desde el inicio de la misma hasta la antepenúltima palabra acaba en \texttt{"br","i","j","t" o "zn"}, y si es así se quita la última letra (suele ser una \textsl{s}). Posteriormente se itera sobre una lista de terminaciones comunes que son irregulares, por ejemplo, "unes" $\rightarrow$ ún (comunes $\rightarrow$ común) tratando de encontrar si la palabra es uno de esos casos. Finalmente, se añaden cuatro reglas más: \newline
Si la palabra acaba en \texttt{"esis","osis","isis"} se deja como está (esclerosis, hipótesis, electrólisis...). Si acaba en \texttt{"ces"}, se quitan las tres últimas y se añande \texttt{"z"} (luces $\rightarrow$ luz, perdices$\rightarrow$ perdiz, felices $\rightarrow$ feliz...). Si la palabra acaba en \texttt{"es"} se elimina ésta terminación de la palabra (hospitales $\rightarrow$ hospital, caudales $\rightarrow$ caudal) y si acaba en \texttt{"s"}, se elimina ésta (datos $\rightarrow$ dato).     
El proceso en código es: 
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def singularize(word: String, pos:String ):String={
    val w=word.toLowerCase()
    //los gatos=> el gato
    if (pos=="DT"){
      if(List("la","las","los").contains(w)) return "el"
      if(List("una","unas","unos").contains(w)) return "un"
    }
    //hombres=>hombre
    if (w.endsWith("es") && (w.substring(0,w.length-2).endsWith("br") || (w.substring(0,w.length-2).endsWith("i")) ||
      (w.substring(0,w.length-2).endsWith("j")) || (w.substring(0,w.length-2).endsWith("t")) || (w.substring(0,w.length-2).endsWith("zn"))))
      return w.substring(0,w.length-1)
    //gestiones=>gestión
    val endings=List(("anes", "án"),
      ("enes", "én"),
      ("eses", "és"),
      ("ines", "ín"),
      ("ones", "ón"),
      ("unes", "ún"))
    endings.foreach(e=> {if(w.endsWith(e._1)) return w.substring(0,w.length-4).concat(e._2)})
    //hipotesis=>hipotesis
    if (w.endsWith("esis") || w.endsWith("osis") || w.endsWith("isis")) return w
    //luces=>luz
    if(w.endsWith("ces")) return w.substring(0,w.length-3).concat("z")
    //hospitales=>hospital
    if (w.endsWith("es")) return w.substring(0,w.length-2)
    //gatos=>gato
    if (w.endsWith("s")) return w.substring(0,w.length-1)

   else return w
  }
\end{minted}

\subsection{Predicative}
Éste método singulariza los adjetivos. Para ello se aplican cuatro casos: si la palabra acaba en \texttt{"os","as"}, se elimina la \textsl{s} final (histéricos $\rightarrow$ histérico, agobiados $\rightarrow$ agobiado). Si acaba en \texttt{"o"}, se deja igual, pero si acaba en \texttt{"a"}, ésta se cambia por una   \texttt{"o"} (agobiada $\rightarrow$ agobiado). Por último, si la palabra acaba en \texttt{"es"}, tiene más de 4 letras y las letras en posición $3^{\circ}$ y $4^{\circ}$ por la cola son consonantes se elimina la última letra de la misma (horribles $\rightarrow$ horrible). En caso contrario, se eliminan las dos últimas (humorales $\rightarrow$ humoral, sociales $\rightarrow$ social). El proceso en código se decribe como sigue:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def predicative(word:String):String={
  var w=word.toLowerCase()
    //histéricos=>histérico
    if (w.endsWith("os") || w.endsWith("as")) w=w.substring(0,w.length-1)
    // histérico=>histérico
    if (w.endsWith("o")) w= w
    //histérica=>histérico
    if(w.endsWith("a")) w= w.substring(0,w.length-1).concat("o")
    //horribles=>horrible, humorales=>humoral
    if(w.endsWith("es")){
      if (w.length >= 4 && !(isVowel(normalize(w.charAt(w.length - 3)))) && !(isVowel(normalize(w.charAt(w.length - 4))))) w= w.substring(0, w.length - 1)
      else w= w.substring(0,w.length-2)
    }
    return w
  }
\end{minted}

\subsection{Lematización de verbos}
Para lematizar una forma verbal, se comprueba si esta está en el diccionario interno de (forma verbal, lema) de la clase (\textcolor{SchoolColor}{mappedVerbs}). Si se encuentra, se devuelve su lema asociado. Si no se encuentra, se inicia un proceso de lematización del verbo basado en reglas como sigue:
\begin{itemize}
\item Si el verbo acaba en \texttt{"ar","er","ir"} es un infinitivo y por tanto esa es la forma base
\item Se definen en la clase una serie de inflexiones regulares típicas como:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
private[this] val irregular_inflections=List(
    ("yéramos", "ir"   ), ( "cisteis", "cer"   ), ( "tuviera", "tener"), ( "ndieron", "nder" ),
    ( "ndiendo", "nder" ), ("tándose", "tarse" ), ( "ndieran", "nder" ), ( "ndieras", "nder" ),
    ("izaréis", "izar" ), ( "disteis", "der"   ), ( "irtiera", "ertir"), ( "pusiera", "poner"),
    ( "endiste", "ender"), ( "laremos", "lar"   ), ("ndíamos", "nder" ), ("icaréis", "icar" )
    ....
\end{minted}
y si coincide alguna con el verbo a lematizar, se le aplica.
\item  Si el verbo a lematizar contiene \texttt{"zco", "zca", "zcá"} ésto se reemplaza por \texttt{"ce"}. Por ejemplo, conozco $\rightarrow$ conoce. Ésto se hace porque luego se tratan con una regla los verbos en $3^{\circ}$ persona.
\item Si el verbo contiene \texttt{"ldr" o "ndr"}, por ejemplo \textsl{valdrá, contendrá, compondrá, pondrá, mantendrá...} se mantiene la \texttt{"l" o "n"} respectivamente, y el \texttt{"dr"} en adelante se cambiará por \texttt{"er"}. Obteniendo  \textsl{valer, contener, componer, poner, mantener...}.
\item Muchos verbos cuyo infinitivo acaba en \textsl{-ar} tienen inflexiones regulares. Por ejemplo, \texttt{ cantabas, cantó, presentaremos, invitarás, ahorraríais...} cuyas inflexiones son \texttt{"as","ó","aremos","arás","aríais"}. En este paso se define una lista de inflexiones regulares para verbos acabados en \textsl{-ar} y se aplica. Si coincide alguna, se elimina la inflexión y se añade el infinitivo. 
\item Parecido al paso anterior pero con una lista de inflexiones regulares para verbos que acaben en \textsl{-er}.
Sin embargo, muchas de estas inflexiones también son comunes en verbos cuyo infinitivo acaba en \textsl{-ir}. Para detectar de que conjugación es, se comprueba si la palabra sin la inflexión es mayor que dos y si la penúltima letra de ésta es \texttt{"i"}. En ese caso, Se le añade el infinitivo \texttt{"-ir"}. En caso contrario, se le añade \texttt{"-er"}. Por ejemplo, \textsl{"correríais"} quedaría como \textsl{"corr-"} sin inflexión, y eso tiene longitud mayor que dos. Pero como su penúltima letra es una \texttt{"r"}, se lematizaría como \texttt{"correr"}.
\item Se aplica un proceso exactamente igual al de los infinitivos acabados en \texttt{"-ar"} pero con inflexiones típicas de verbos con infinitivos acabados en \texttt{"-ir"}.
\item Las palabras acabadas en \texttt{"a"} u \texttt{"o"} son consideradas de la primera conjugación, por lo que se elimina dicha terminación y se les añade \texttt{-ar}.
\item Igual que el anterior para verbos acabados en  \texttt{"as", "an"}.
\item Para verbos presente, $3^{\circ}$ persona del singular acabados en \texttt{"-e"} se comprueba si la palabra sin la inflexión es mayor que dos y si la antepenúltima letra de ésta es \texttt{"i"}, y en ese caso se elimina \texttt{"-e"} para añadirle \texttt{"-ir"}. En caso contrario, se eliminaría \texttt{"-e"} para añadir \texttt{"-er"}.
\item Se hace un proceso similar a los dos anteriores pero para las terminaciones \texttt{"-es", "-en"}.
\item Por último se crea una lista de inflecciones verbales de tiempos en presente para $1^{\circ}$ y $2^{\circ}$ personas del plural, asociándole las conjugaciones a las que suelen pertenecer con más frecuencia:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
  val present_plural_inflection_o=List(
    ("amos", "áis"),
    ("emos", "éis"),
    ("imos", "ís")
  )
  val terminations=Array("ar","er","ir")
\end{minted}
 y se itera sobre estas inflexiones, aplicando la conjunción correspondiente al verbo a lematizar si éste presenta alguna terminación que case con las de la lista de inflexiones.
\end{itemize}
A continuación se ilustra el proceso en código Scala:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def find_lemma(verb:String):String={
  // Spanish has 12,000+ verbs, ending in -ar (85%), -er (8%), -ir (7%).
  // Over 65% of -ar verbs (6500+) have a regular inflection.
  var v = verb.toLowerCase
  if(verb.endsWith("ar") || verb.endsWith("er") || verb.endsWith("ir")) return verb //verb is infinitive
  //set of rules for irregular inflections +10%

  irregular_inflections.foreach(a=> if(v.endsWith(a._1)) return v.substring(0,v.length-a._1.length).concat(a._2))
  //reconozco=>reconocer
  v=v.replace("zco","ce")
  //reconozcamos=>reconocer
  v=v.replace("zca","ce")
  //reconozcáis=>reconocer
  v=v.replace("zcá","ce")
  //valdrá => valer
if(v.contains("ldr")) return v.substring(0,v.indexOf("ldr")+1).concat("er")
  //contendrá=>contener
  if(v.contains("ndr")) return v.substring(0,v.indexOf("ndr")+1).concat("er")
//many verbs end in -ar and have a regular inflection
val regular_inflection_ar=List (
  "ando", "ado", "ad",                                // participle
  "aré", "arás", "ará", "aremos", "aréis", "arán", // future
  "aría", "arías", "aríamos", "aríais", "arían",    // conditional
  "aba", "abas", "ábamos", "abais", "aban",         // past imperfective
    "é", "aste", "ó", "asteis", "aron",               // past perfective
    "ara", "aras", "áramos", "arais", "aran")       // past subjunctive

regular_inflection_ar.foreach(u=> if (v.endsWith(u)) return v.substring(0,v.length-u.length).concat("ar"))
//Many verbs end in -er and have a regular inflection
  val regular_inflection_er=List(
      "iendo", "ido", "ed",                               // participle
  "eré", "erás", "erá", "eremos", "eréis", "erán", // future
  "ería", "erías", "eríamos", "eríais", "erían",    // conditional
  "ía", "ías", "íamos", "íais", "ían",              // past imperfective
    "í", "iste", "ió", "imos", "isteis", "ieron",        // past perfective
    "era", "eras", "éramos", "erais", "eran")       //past subjunctive

  regular_inflection_er.foreach(u=>if(v.endsWith(u)){
    val difLength=v.length-u.length
    if(v.substring(0,difLength).length>2 && v.substring(0,difLength).charAt(difLength-2)=='i')
  return v.substring(0,difLength).concat("ir") else return v.substring(0,difLength).concat("er")} )

  //Many verbs end in -ir and have a regular inflection
  val regular_inflection_ir=List(
    "iré", "irás", "irá", "iremos", "iréis", "irán", //future
  "iría", "irías", "iríamos", "iríais", "irían")  //past subjunctive

  regular_inflection_ir.foreach(u=>if(v.endsWith(u)) return v.substring(0,v.length-u.length).concat("ir"))
  //Present 1sg -o: yo hablo, como, vivo => hablar, comer, vivir.
  if(v.endsWith("o")) return v.substring(0,v.length-1).concat("ar")
  //Present 2sg, 3sg and 3pl: tú hablas.
  if(v.endsWith("a")) return v.substring(0,v.length-1).concat("ar")

  if(v.endsWith("as") || v.endsWith("an")){
    if (v.endsWith("as")) v =v.stripSuffix("s").dropRight(1)
    else if( v.endsWith("an")) v=v.stripSuffix("n").dropRight(1)
   return v.concat("ar")
  }
  // Present 2sg, 3sg and 3pl: tú comes, tú vives.
  if(v.endsWith("e")){
    if(v.substring(0,v.length-1).length>2 && v.substring(0,v.length-1).charAt(v.length-3)=='i') return v.substring(0,v.length-1).concat("ir")
    else return v.substring(0,v.length-1).concat("er")
  }
  if(v.endsWith("es") || v.endsWith("en")){
    if (v.endsWith("es")) v =v.stripSuffix("s").dropRight(1)
    else if( v.endsWith("en")) v=v.stripSuffix("n").dropRight(1)
    if(v.length>2 && v.charAt(v.length-2)=='i') return v.concat("ir")
  else return v.concat("er")
  }
  // Present 1pl and 2pl: nosotros hablamos.

  val present_plural_inflection_o=List(
    ("amos", "áis"),
    ("emos", "éis"),
    ("imos", "ís")
  )
  val terminations=Array("ar","er","ir")
  //terminations{1}
  present_plural_inflection_o.foreach(u=> {
    if (v.endsWith(u._1)) return v.substring(0, v.length - u._1.length).concat(terminations {
      present_plural_inflection_o.indexOf(u)
    })

    if (v.endsWith(u._2)) return v.substring(0, v.length - u._2.length).concat(terminations {
      present_plural_inflection_o.indexOf(u)
    })

  })

  return v
}

\end{minted}

\section{Parser}
Por último, estos tres procesos se integran en una clase \textcolor{SchoolColor}{Parser}, donde se define un método \textcolor{SchoolColor}{parse} con la siguiente cabecera:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
def parse(text: String, tokenize: Boolean, tags: Boolean, lemmatize: Boolean ,mapCall:(String,String)=>(String,String)): String
\end{minted}
Recibe como parámetros el texto a parsear, tres booleanos indicando si se desea hacer \newline tokenización, pos etiquetado y lematización y una función para mapear las etiquetas al tagset \newline deseado.

\section{Pruebas}
Las pruebas realizadas son todos los test unitarios incluidos en la clase \textcolor{SchoolColor}{AppTest}. Para los Test se usa la librería \textsf{ScalaSuite} y el estilo de test \textsf{FunSuite}. En los Test se emplean aserciones generales para hacer las comprobaciones (assert). En la clase \textcolor{SchoolColor}{AppTest} se incluyen toda la declaración de variables necesarias para los Test y Test implementados. Se implementan test sólo para los métodos más importantes, implementando un test por cada funcionalidad relevante. Los test más destacados son:
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
test("test find tags"){
     var wikicorpus=List[List[String]]()
     var i=0
     var n=0
     
     // Assert the accuracy of the Spanish tagger.

     val lexiconSpanish=Lexicon.read("../Spanish_Lematizer/data/es-lexicon.txt","utf-8",";;;")
     val morphologySpanish=morphology.read("../Spanish_Lematizer/data/es-morphology.txt","utf-8",";;;")
     val contextSpanish=context.read("../Spanish_Lematizer/data/es-context.txt","utf-8",";;;")

     scala.io.Source.fromFile("../Spanish_Lematizer/corpus/tagged-es-wikicorpus.txt").getLines().foreach(line => wikicorpus ::= line.split(" ").toList)
     wikicorpus=wikicorpus.reverse
     wikicorpus.foreach(sentenceList=>{
       var s1= sentenceList.map(f=>f.split("/"))
       var s1ToTag=List[String]()
       s1.foreach(f=> s1ToTag::=f(0))
       s1ToTag=s1ToTag.reverse
       val tagged=tagger.find_tags(s1ToTag, Lexicon, morphology,context, List("NCS","NP","Z"), null)

       var j=0
       s1.foreach(s=>{
         if(s(1)==tagged(j)._2) i=i+1
         n=n+1
         j=j+1
       })
     })
     assert(i.toFloat/n>0.91)
     print("accuracy tagger:" + i.toFloat/n)
   }
}
\end{minted}
Donde se calcula el porcentaje de acierto del método de pos etiquetado implementado sobre el corpus etiquetado \textcolor{SchoolColor}{Wikicorpus} \url{http://www.cs.upc.edu/~nlp/wikicorpus/}. 

\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
test("singularize") {
     var wordforms = List[List[String]]()
     var testDict = mutable.Map.empty[String, List[String]]
     //read file
     scala.io.Source.fromFile("../Spanish_Lematizer/test/data/wordforms-es-davies.csv").getLines().foreach(line => wordforms ::= line.split(" ").toList)
     wordforms.reverse.foreach(l => {
       val splitted = l(0).split(",").map(u => u.tail.stripPrefix("\"").stripSuffix("\""))
       val w = splitted(0)
       val lemma = splitted(1)
       val tag = splitted(2)
       val f = splitted(3)

       if (tag == "n") {
         testDict += (lemma -> (w +: testDict.getOrElse(lemma, List.empty)))
       }
     })

     var i = 0
     var n = 0
     testDict.foreach(f => {
       if (lemmatizer.singularize((f._2.sortWith(_.length < _.length)).head, "NN") == f._1) i = i + 1
       n = n + 1
     })
     assert(i.toFloat / n > 0.93)
     print("singularize accuracy:" + i.toFloat/n )
   }
\end{minted} 
Donde se calcula el porcentaje de acierto para el método \textcolor{SchoolColor}{singularize} sobre 3000 palabras etiquetadas con su lema obtenidas del corpus \textsf{wordforms-es-davies.csv} \url{http://www.wordfrequency.info/files/spanish/spanish_lemmas20k.txt}.
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
test("predicative") {
     var wordforms = List[List[String]]()
     var testDict = mutable.Map.empty[String, List[String]]
     //read file
     scala.io.Source.fromFile("../Spanish_Lematizer/test/data/wordforms-es-davies.csv").getLines().foreach(line => wordforms ::= line.split(" ").toList)
     wordforms.reverse.foreach(l => {
       val splitted = l(0).split(",").map(u => u.tail.stripPrefix("\"").stripSuffix("\""))
       val w = splitted(0)
       val lemma = splitted(1)
       val tag = splitted(2)
       val f = splitted(3)

       if (tag == "j") {
         testDict += (lemma -> (w +: testDict.getOrElse(lemma, List.empty)))
       }
     })
     var i = 0
     var n = 0
     testDict.foreach(f => {
       if (lemmatizer.predicative(f._2.sortWith(_.length < _.length).head) == f._1) i = i + 1
       n = n + 1
     })
     assert(i.toFloat / n > 0.92)
     print("predicative accuracy:" + i.toFloat/n  )
   }
\end{minted}
Donde se testea el método \textcolor{SchoolColor}{predicative}, obteniendo su porcentaje de acierto sobre el mismo corpus.
\begin{minted}
[frame=lines,
framesep=1.3mm,
fontsize=\footnotesize,
breaklines=true
]{scala}
test("find lemma") {
     var i = 0
     var n = 0
     var faults = List[(String, String)]()
     lemmatizer.getVerbsDict.foreach(u => {
       if (lemmatizer.find_lemma(u._1) == u._2) i = i + 1 else faults ::= u
       n = n + 1
     })

      assert(i.toFloat/n > 0.80)
      print("accuracy lematization of verbs: " + i.toFloat/n )
}
\end{minted} 
Por último, el test donde se comprueba que la implementación de lematización de verbos obtiene la precisión de etiquetado deseada. Para ello se testea sobre las 23435 entradas de verbos del diccionario creado a partir de \textsf{es-verbs.txt}.
Los resultados obtenidos se discuten en el capítulo siguiente.