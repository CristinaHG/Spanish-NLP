%!TEX root = ../dissertation.tex
\chapter{Introducción y motivación}
\label{introducción}
\section{Introducción al PLN}
El proyecto desarrollado se engloba en el campo del Procesamiento del lenguaje natural. El lenguaje natural es cualquier lenguaje usado por los humanos para comunicarse (Alemán, Inglés, Español, Hindi...). Dado que estos lenguajes se transmiten entre generaciones y van experimentando evoluciones, resulta difícil obtener reglas que los describan. (PLN) es, por tanto, el área de estudio y aplicación que engloba cualquier tipo de manipulación computacional del lenguaje natural.\newline Es decir, PNL abarca desde aplicaciones simples, como contar el número de ocurrencias de las palabras en un texto para comparar diferentes estilos de escritura, a aplicaciones más complejas, como comprender expresiones humanas completas para poder dar respuestas útiles a preguntas. \citet{bird2009natural} Como por ejemplo, el asistente Siri de iPhone. 

La lingüística computacional o PLN comenzó en 1980, sin embargo en los últimos 20 años ha crecido enormemente, despertando un gran interés en el ámbito de la investigación científica pero también en el ámbito práctico, ya que cada vez son más los productos, especialmente los tecnológicos, que incorporan algún tipo de aplicación basada en NLP. Por ejemplo, traductores como el traductor de Skype, o asistentes de voz inteligentes (Cortana de Microsoft, Google Now de Google o el ya mencionado Siri de Apple). \newline
Este crecimiento en el campo del procesamiento del lenguaje natural se debe principalmente a que en los últimos años, con el uso de redes sociales como Facebook, SnapChat, Twitter, Google plus, Linked in... y de sitios web comerciales como Amazon o Booking, los usuarios han generado una gran cantidad de contenido mayoritariamente subjetivo, el cual se puede aplicar en muchos ámbitos como márketing, política, gestión de crisis, soporte, atención al cliente, etc. También han influido en su crecimiento el aumento de capacidad de procesamiento y cómputo que ha habido en los últimos años y el desarrollo de técnicas de machine learning más complejas y potentes.    
\newline

En la actualidad, según lo descrito en \citet{hirschberg2015advances} éstas son algunas de las principales áreas en PLN:
\section*{Traducción automática}
La traducción automática es el área del PLN que tiene como objetivo el empleo de sofware para ayudar a traducir de un lenguaje natural a otro, ya sea en texto o hablado. Ésto supone una gran dificultad, ya que para que una traducción sea correcta, no basta con traducir palabra a palabra, sino que hay que tener en cuenta el sentido de la palabra y el contexto de ésta, pues hay casos en los que la misma palabra significa varias cosas dependiendo del contexto. Por ejemplo, en las frases \textsl{"compra una lata de refresco"} y \textsl{"deja ya de dar la lata"} aparece la  palabra \textsl{lata} desempeñando una  función distinta:\newline En la primera frase, \textsl{lata} es un nombre, por lo que se entendería como un envase hecho de hojalata, mientras que en la segunda frase aparece como una locución verbal, por lo que se entendería como "molestar" o "importunar". \newline  
El campo de la traducción automática se empezó a estudiar a finales de 1950s, sin embargo inicialmente no tuvo mucho éxito debido a que los traductores construidos eran sistemas basados en gramáticas escritas a mano. Fue a partir de 1990 y gracias a que los científicos de IBM consiguieron una cantidad suficientemente grande de frases de traducciones entre dos lenguajes, cuando construyeron un modelo probabilístico de traducción automática.\newline
A partir de entonces se siguió investigando y se descubrieron los \textcolor{SchoolColor}{traductores máquina basados en frases}, que en lugar de ir traduciendo palabra a palabra, detectaban los pequeños subgrupos de palabras que solían ir juntas y que tenían una traducción especial. Esto se utilizó para desarrollar el traductor de Google.  
\newline
Actualmente, el estado del arte en este campo está en traductores máquinas que usan deep learning, entrenando un modelo de varios niveles para optimizar un objetivo (la calidad de la traducción), donde luego el modelo pueda aprender por sí mismo más niveles que le sean útiles para desarrollar la tarea. Esto ha sido estudiado especialmente para redes neuronales, habiendo conseguido en varios casos obtener los mejores resultados hasta el momento, empleando redes neuronales distribuidas. Como por ejemplo, en  \citet{luong2014addressing}.

\section*{Sistemas de reconocimiento del habla}

Esta área, muy conocida desde 1980s, estudia como permitir y mejorar la comunicación entre humanos y máquinas. Aunque siempre se ha pensado, por ejemplo, en aplicaciones como robots que ayudan en casa o a personas con movilidad reducida,no muchos años atrás se expandió al ámbito de los smartphones (mencionábamos en la introducción a los asistentes de voz para móvil más conocidos). \newline
El reconocimiento del habla necesita principalmente de :
\begin{itemize}
\item Una herramienta de reconocimiento automático del habla (RAH) para identificar que está diciendo el humano.
\item Una herramienta de manejo de diálogo (MD) para identificar lo que quiere el humano.
\item Acciones para realizar la actividad solicitada.
\item Una herramienta de síntesis texto a voz para que la máquina pueda comunicar al humano el resultado de forma hablada.
\end{itemize}
Sin embargo, aún se está investigando como hacer estas herramientas más precisas. Añadiéndole a lo anterior las dificultades propias de reconocer lenguaje humano hablado: pausas, coletillas, coordinación, toma de turnos... desemboca en que los sistemas de reconocimiento de habla aún no han tenido gran éxito interactuando en dominios abiertos, donde los usuarios pueden hablar de cualquier cosa, aunque en dominios cerrados donde conocían el tema han mostrado resultados mejores. \newline
En los últimos años se ha aplicado deep learning en estos sistemas, mapeando señales de sonido a secuencias de palabras y sonidos del lenguaje humano \citet{hinton2012deep}, aunque actualmente el
enfoque más usado es el proceso de decisión de Markov, que hace identificación del diálogo (pregunta, sentencia, acuerdo..) mediante una probabilidad de distribución sobre todos los posibles estados del sistema, que va actualizando según se desarrolla el diálogo. \citet{young2013pomdp}.

\section*{Lectura Automática}
La lectura automática es el área que tiene como objetivo que las máquinas puedan integrar o resumir información a los humanos, mediante la lectura y comprensión de las grandes cantidades de texto disponibles. \newline
Esta idea atrae especialmente a los científicos, ya que es complicado llevar el ritmo de todas las publicaciones que se hacen, aunque sólo sea en su campo, por lo que sería de gran utilidad que un sistema pudiera resumir e identificar los datos más relevantes de las publicaciones. 
El objetivo inicial de estos sistemas es la extracción de relaciones, es decir, ser capaz de extraer relaciones entre dos entidades, como por ejemplo "A es hermano de B", lo cual ya se ha realizado con éxito en dominios específicos. Aunque hay técnicas que escriben los patrones de las relaciones a mano (por ejemplo: <PERSONA>, el hermano de <PERSONA>), se ha demostado que aplicando Machine learning se obtienen mejores resultados, ya que se pueden obtener relaciones basadas en características extraidas de secuencias de palabras y secuencias gramaticales de una frase. \citet{culotta2004dependency}.
\newline
Los sistemas más recientes han usado inferencia probabilística sofisticada para distinguir qué claúsulas textuales se asocian a qué factores de la base del conocimiento, por ejemplo,  \citet{niu2012deepdive} y apuestan por técnicas de extracción de hechos más simples pero más escalables que no requieren etiquetado manual de los datos, o las extraen usando NLP. \citet{etzioni2011open}. 

     
\section*{Minería de datos en medios sociales}
La minería de datos es el campo que tiene como objetivo descubrir patrones en grandes volúmenes de datos. Hoy en día, la gran cantidad de datos disponibles a través de redes sociales (Facebook, Twitter, Instagram, Youtube..), blogs o foros se puede descargar usando técnicas de web scrapping y se usa, aplicando técnicas de machine learning e inteligencia artificial, para aprender a detectar información demográfica a partir del lenguaje (como sexo o edad), hacer un seguimiento de las tendencias más populares u opiniones más populares sobre política o sobre productos, e incluso, como hizo Google (\url{www.google.org/flutrends/}) para ver como se difunde la gripe a través de los tweets de los usuarios y sus búsquedas en internet \citet{elhadad2014information}. \newline
A pesar de que este campo tiene innumerables aplicaciones, muchas de las cuales podrían ser de gran interés (como por ejemplo, detectar grupos que hacen bullying a otros o fomentan el odio), están aumentando los poblemas de privacidad y se está limitando el acceso a esos datos. Por ejemplo, Twitter          ya ha limitado el periodo de tiempo del que se pueden descargar tweets. Instagram también a modificado su API con este propósito. \newline
Otra dificultad  con la que cuenta este campo, es la validación. Muchas veces no hay forma de comprobar que la información presente en internet es cierta, por ejemplo las reseñas sobre hoteles, productos o restaurantes. En la actualidad, Facebook está ideando un modelo para detectar noticias falsas en su red social. Aunque se ha probado a agregar información de distintas fuentes para intentar validar la información, de momento no ha tenido mucho éxito.    
\section*{Análisis de sentimientos}
Este campo (también conocido como minería de opiniones) analiza las opiniones, sentimientos, valoraciones, actitudes y emociones de la gente frente a entidades como productos, servicios, organizaciones, individuos, eventos, temas, cuestiones...\newline
\citet{9781107017894} emplea el término \textsl{opinión} para referirse al concepto de sentimento, evaluación, valoración o actitud e información asociada (objetivo de la opinión o persona que da la opinión) en su totalidad, y el término \textsl{sentimiento} para referirse al sentido positivo o negativo subyacente en una opinión. Por ejemplo \textsl{"Apple lo está haciendo muy bien en esta economía pobre"} es una opinión que contiene dos sentimientos, uno positivo con Apple como objetivo y otro negativo sobre la economía actual.\newline
 Los estudios sobre este campo comenzaron en el año 2000, principalmente debido a que para entonces se empezó a recoger texto subjetivo en formato digital. Actualmente hay muchos campos relacionados con este cuyas tareas difieren ligeramente, por ejemplo análisis de opiniones, análisis de subjetividad, minería de sentimientos... aunque gran parte del trabajo se concentra en el análisis de sentimientos. \newline
Los enfoques más simples tratan de identificar si lo expresado en el texto (por ejemplo, en un tweet) tiene una orientación positiva o negativa usando dicccionarios de sentimientos como \citet{whissell1989dictionary}. Otros enfoques más complejos tratan de identificar la polaridad del sentimiento así como el objeto de éste.\citet{wiebe2005annotating}. También se han realizado trabajos recientes tratando de indentificar algunas emociones en particular, como las de Ekman (furia, aversión, miedo, felicidad,tristeza y sorpresa) y se ha investigado sobre reconocer esas emociones clásicas usando características como la edad, la personalidad , el género las condiciones mentales o médicas del usuario.  \citet{hirschberg2015advances}. \newline
Las aplicaciones de este campo son innumerables y abarcan desde identificar valoraciones en productos \citet{wang2015sentiment} a predecir los precios del mercado o evaluar el estado mental de una comunidad. \citet{bollen2011modeling}.
\section{Historia del PLN}
De acuerdo a lo descrito en \citet{hirschberg2015advances}, el PLN comienza en 1980s como intersección entre la inteligencia artificial y la lingüística.  
Durante las primeras décadas, los investigadores escribían a mano las reglas y el vocabulario del lenguaje humano. Sin embargo, no se obtuvo éxito, debido a la variabilidad y dificultad del lenguaje humano. Por ejemplo, el traductor palabra a palabra de ruso a inglés que no tenía en cuenta el contexto, el léxico o la morfología y que tradujo la frase bíblica : \textsl{"El espíritu está dispuesto, pero la carne es débil"} como \textsl{"El vodka es agradable, pero la carne es
estropeado"} según \citet{nadkarni2011natural}. \newline
Es a partir de 1990 cuando el PLN sufre una transformación cuando los investigadores comienzan a tener la posibilidad de obtener grandes cantidades de datos del lenguaje en formado digital y construyen modelos sobre estos. Surge así el PLN estadístico o PLN basado en corpus, lo que supuso un éxito en el uso del "big data", aunque ese término se introduciría más adelante. Con lo anterior surgen métodos que usan el \textsc{Part-Of-Speech} (POS) de las palabras, es decir, su categoría morfosintáctica( si son sustantivo, un adjetivo, un verbo, una preposición...) adquiriendo notables resultados cuando se entrena con un conjunto de datos suficientemente grande. \newline
Actualmente, muchos clasificadores de texto y sentimientos se basan únicamente en los diferentes conjuntos de palabras que presenta el texto (bolsas de palabras) sin tener en cuenta estructuras a nivel de frase, de documento o de significado. Sin embargo, los mejores enfoques actuales usan técnicas sofisticadas de machine learning y un buen entendimiento de la estructura lingüística subyacente, identificando información sintáctica, semántica y de contexto. Por ejemplo, Standford CoreNLP \citet{manning2014stanford} o python NLTK \citet{bird2006nltk}. 

\section{Aproximaciones más comunes en PLN} 
Debido a la gran aplicabilidad que tiene el análisis de sentimientos, se ha despertado un gran interés 
por este área del PLN, proponiendo en los últimos años numerosas aproximaciones que emplean varias técnicas desde diferentes áreas de la informática para resolver el problema. \newline
En la actualidad, las aproximaciones más comunes son dos \citet{ribeiro2016sentibench}: 
\subsection{Aplicación de métodos de machine learning supervisados}
Estos enfoques aplican métodos de clasificación supervisada, por lo que tienen como desventaja que requieren de datos (corpus) etiquetados (con información morfosintáctica, lema...) para entrenar los clasificadores, lo que resulta costoso en tiempo dado que son cantidades grandes de datos.\newline
De esta forma se consigue que los algoritmos aprendan de los datos etiquetados para luego ser capaces de clasificar otros datos de entrada. Como ventaja, estos métodos tienen la habilidad de adaptar y crear modelos entrenados para objetivos y contextos concretos.  
\subsection{Aplicación de métodos basados en léxico}
Estos enfoques tienen en común que emplean listas predefinidas de palabras, donde cada palabra está asociada a un sentimiento específico. Los métodos basados en léxico varían según el contexto en el que se crean, por ejemplo, \textsc{LIWC} \citet{tausczik2010psychological} fue originalmente propuesto para analizar patrones de sentimientos en textos formales escritos en inglés, mientras que \textsc{PANAS-t} \citet{gonccalves2013panas} fue adaptado al contexto web.\newline
A pesar de que no tienen la parte negativa de necesitar un conjunto de datos etiquetados, tienen la dificultad de crear un diccionario basado en léxico que sea aplicable en múltiples contextos. 
\section{Estado del arte}
En la sección anterior se describían las aproximaciones más comunes actualmente en el análisis de sentimientos. Ya que tenemos una idea, en esta sección se ilustra brevemente el estado del arte.\newline
En investigación, el análisis de sentimientos se ha desarrollado a tres niveles, según  \citet{9781107017894} y \citet{westerski2007sentiment} :
\subsection{Análisis de sentimientos a nivel de documento}
El objetivo de este análisis es clasificar todos los sentimientos expresados por los autores a lo largo del documento, concluyendo si lo expresado en el documento es positivo, negativo o neutro, sobre una entidad que puede ser un producto o servicio. La mayoría de las técnicas de análisis de sentimientos a nivel de documento obtienen un acierto de clasificación entre el 70\% y el 80\%,cuando son aplicadas a un solo tipo de texto, dependiendo de la cantidad de texto que se tenga como entrada y del tipo de texto. \newline
Algunos de las soluciones más destacadas en este área, son:\newline
Por una parte, el trabajo desarrollado por  \citet{turney2002thumbs} para clasificación de críticas, donde presenta un algoritmo de tres pasos que procesa los documentos sin supervisión humana. Este se basa en la medida de la distancia entre los adjetivos encontrados en el texto a palabras preseleccionadas con polaridad conocida. Brevemente, los pasos que sigue son:\newline
\begin{itemize}
\item Paso 1: Extrae los adjetivos del documento aplicando una serie de patrones predefinidos ( como nombre-adverbio, nombre-adjetivo...etc)
\item Paso 2: Mide la orientación semántica. Para ello se mide la distancia a palabras cuya polaridad se conoce ("excelente" y "pobre"). Obtiene la dependencia entre dos palabras analizando el número de ocurrencias con el motor de búsqueda \textcolor{SchoolColor}{AltaVista} para documentos que contienen dos palabras relativamente próximas. 
\item Paso 3: Finalmente cuenta la orientación semántica media para todos los pares de palabras y  clasifica la crítica como recomendada o no recomendada. 
\end{itemize}

Por otro lado, \citet{pang2002thumbs} presentaron otro trabajo basándose en técnicas conocidas de clasificación. En la propuesta se testea un grupo seleccionado de algoritmos de aprendizaje automático, comprobando si producen buenos resultados cuando se aplica el análisis de sentimientos a nivel de documento.\newline
Presentan resultados para \textsf{Naive Bayes} \citet{lewis1998naive}, \textsf{Máxima entropía} \citet{berger1996maximum} y \textsf{Máquinas de soporte vectorial} \citet{joachims1998text}, con un acierto de clasificación final de 71\%-85\%, dependiendo de la técnica y el conjunto de datos empleados.       





\section{El pipeline Genérico}
El objetivo final de toda suite de PLN es tener un sofware que contenga un conjunto de herramientas para poder realizar análisis de sentimientos o algún tipo de extracción de información. Esto se alcanza a través de varias tareas de procesamiento del texto, empezando por hacer el contenido uniforme y acabando por identificar las funciones de las palabras y como se organizan. Aquí se describen las tareas más comunes del pipeline de un software de PLN junto con las aproximaciones más comunes. \newline
\subsection{Obtención de datos}
La obtención de datos es la primera tarea a realizar, mediante la cual se obtendrá el corpus de texto, etiquetado o no (depende de la aproximación a aplicar) que se va a usar para crear la herramienta de PLN, así como los datos sobre los que se quiere hacer el análisis de sentimientos o la extracción de información deseada. \newline
La forma general de obtener los datos directamente de internet es a través de la propia API de los sitios web de los que se desee descargar información por ejemplo la Api de twitter si queremos descargar tweets, o la api de instagram para imágenes y etiquetas. Sin embargo, a pesar de que hay manuales que explican como consultarlas, estas APIs  han limitado mucho las descargas últimamente, alegando cuestiones de privacidad de los datos de usuario. Otro método es usar arañas web para rastrear la información deseada de forma similar a lo que hace google con su conocida araña \textsc{Googlebot}, con la que visita todas las webs frecuentemente para añadirlas a su índice y percatarse de los cambios. Ahora bien, este último método requiere unos conocimientos necesarios para crear una araña web y ocupa mucho ancho de banda. \newline
En cuanto a corpus ya etiquetados en inglés se pueden encontrar muchos, entre los más populares están \textsc{The Penn Treebank} \citet{marcus1993building} o \textsc{Wikicorpus} \url{http://www.cs.upc.edu/~nlp/wikicorpus/}, mientras que para español son conocidos \textsc{Wikicorpus}, \textsc{Ancora} \url{http://clic.ub.edu/ancora} o \textsc{SemEval} \citet{marquez2007semeval}. 
    
\subsection{Procesamiento del texto}
Teniendo el texto, se le aplica un procesamiento, separándolo en unidades relevantes, añadiendo información sintáctica y morfológica, extrayendo entidades y relaciones entre estas... En este apartado se describen las fases de procesamiento más comunes según describe \citet{9783319155623}.
\subsubsection{Tokenización}
En esta fase se separa el texto del documento en sus unidades atómicas, los tokens (palabras, números, símbolos), por lo que es una fase totalmente necesaria en casi cualquier suite de PLN. Aunque no es una tarea muy compleja para lenguajes que emplean espacios entre palabras, como la mayoría de lenguajes que usan el alfabeto latino, sí que resulta mucho más complicado en lenguajes que no usan espacios entre palabras, como el Chino. \citet{chang2008optimizing}. \newline
La tokenización se sirve de heurísticas simples, como considerar que todos los strings de caracteres del alfabeto contiguos forman parte del mismo token (igualmente para los números) y que todos los tokens van separados unos de otros por espacios, saltos de línea o por signos de puntuación que no sean abreviaciones. Algunas herramientas actuales que hacen tokenización son \textsf{Freeling} \citet{padro2012freeling}, \textsf{Apache OpenNLP} \citet{baldridge2005opennlp} y los ya citados \textsf{NLTK} y \textsf{StandfordNLP}. No hay ninguna herramienta especialmente dedicada a tokenizar, ya que la tokenización puede ser razonablemente bien hecha usando expresiones regulares cuando se procesan lenguajes que usan el alfabeto latino. Para otros lenguajes más complejos, como el Árabe o el Chino, sí se precisan de tokenizadores más complejos, como el \textsf{Standford Word Segmenter} que aplica segmentación de palabras. 
\subsubsection{Detección de límites de oraciones}
En esta fase se aborda el problema de establecer los límites de una oración en el texto. En algunos casos esto se incluye dentro de la fase de tokenización. Encontrar los límites de una oración no es una tarea trivial, ya que las marcas de puntuación que delimitan el final de una frase suelen ser ambiguas en muchos lenguajes. En español lo son, ya que por ejemplo, el punto se puede usar como marcador de final de oración pero también como separador entre parte entera y decimal en números reales o en iniciales y abreviaciones (por ejemplo, Srta.). \newline
Algunos sisemas que han demostrado buenos resultados en separación de frases sobre diferentes lenguajes naturales, son \textsf{Punkt} \citet{kiss2006unsupervised} e \textsf{iSentenizer}\citet{wong2014isentenizer}.  
\subsubsection{Lematización}
Lematización es el proceso mediante el cual se determina el lema de cada palabra. El lexema de una palabra se define como la unidad mínima que es parte común en todas las palabras de una misma familia. Por ejemplo, \textsl{"arte"} , \textsl{"artístico"} o \textsl{"artista"} son tres palabras distintas que comparten el lexema \textsl{"art"}. Lema es la representación de la forma canónica (o forma de diccionario) de los lexemas, y tiene por tanto significado. Por ejemplo, \textsl{"escribían"}, \textsl{"escribieron"} o \textsl{"escribirán"} son palabras derivadas de un mismo lexema cuyo lema es \textsl{"escribir"}.\newline
Este proceso es importante porque reduce el número de términos a procesar, ya que muchos se reducen al mismo lema, acortando así la complejidad computacional del problema. La dificultad de este proceso varía dependiendo del lenguaje, resultando mucho más sencillo para lenguajes con una morfología inflexiva simple, como el Inglés, y complicándose más para lenguajes morfológicamente más ricos como el Español o el Alemán.  
\subsubsection{Stemming} 
Este método es común en lugar de la lematización e incluso en algunos casos se incluye dentro de ésta o como complemento. El Stemming es un proceso simple mediante el cual se trata de reducir la palabra a su forma base eliminando sus sufijos. Se obtiene así una forma de la palabra que no es necesariamente la raíz de la palabra, pero que suele bastar, dado que palabras de la misma familia tienen esa parte en común, o un conjunto reducido de partes en común, si son palabras irregulares.  Por ejemplo: \textsl{"sleep"}, \textsl{"sleeping"}, \textsl{"sleeped"}, \textsl{"sleeps"}... tienen el común el stem \textsl{"sleep"}, que es la forma base del verbo.
En lenguajes con pocas inflexiones, como el Inglés, es muy probable que el lema y el stem coincidan. Sin embargo, en lenguajes con más inflexiones, como el Español, eso rara vez ocurre.  
\subsubsection{POS Tagging}
POS Tagging es un proceso muy importante en tareas de PLN, ya que etiqueta mediante algoritmos cada palabra con su \textsc{part-of-speech}, es decir, con su categoría morfosintáctica (nombre, verbo, preporisición, adjetivo...) junto a otras propiedades que dependen de esta. De hecho es necesario para realizar el parseo sintáctico (el paso posterior).\newline

Aunque lo usual es aplicar un proceso de lematización o stemming antes de esta fase, hay sistemas que aplican primero esta fase, como el desarrollado en este proyecto.\newline

Los principales desafíos que se presentan en esta etapa, son: 
\begin{itemize}
\item Tratar la ambigüedad, dado que las palabras pueden desempeñar distinta función morfosintáctica y por tanto, tener diferentes POS tags dependiendo del contexto. 
\item Asignar una etiqueta morfosintáctica a palabras de las que el sistema no tiene conocimiento
\end{itemize}  
Para tratar de solventar los problemas anteriores, normalmente se tiene en cuenta el contexto dentro de la frase de la palabra a etiquetar, y se selecciona la etiqueta morfosintáctica más probable dada esa palabra y su contexto. \newline
Que sea una tarea muy necesaria hace que esté muy investigada, por lo que actualmente la precisión de etiquetado ronda el 90\%, aunque esto se debe a que la precisión se mide en porcentaje de palabras bien etiquetadas. Si se midiera por porcentaje de frases completamente bien etiquetadas, la cifra descendería al 55\%-57\% de precisión, como manifestaban \citet{giesbrecht2009part}. \newline
La mayoría de estas herramientas han sido desarrolladas para el Inglés y evaluadas usando los datos de \textsf{Penn Treebank}. Destacan por ejemplo \textsf{Standford POS Tagger} que tiene modelos de etiquetado para 6 lenguajes diferentes, \textsf{SVMTool} \citet{gimenez2004svmtool} que se basa en clasificadores de máquinas de soporte vectorial  y \textsf{TreeTager}, basado en modelos de Markov \citet{schmid1995treetagger}. 
\subsubsection{Parseo Sintáctico}
El parseo sintáctico es una tarea que consiste en analizar las oraciones para producir estructuras que representen como se organizan las palabras en las frases, dada una gramática formal. Las gramáticas tienen dos posibles formalismos estructurales: 
\begin{itemize}
\item Constituyente: es una unidad dentro de una estructura jerárquica que está compuesta por una palabra o un grupo de palabras. \newline
\item De dependencia: estas gramáticas desciben la estructura de una frase en términos de los enlaces entre las palabras, ya que cada enlace refleja una relación de dominancia/dependencia entre un término y otro dependiente de él. Estas gramáticas son las que normalmente se emplean para parsear texto. 
\end{itemize}
Por ejemplo, para la frase \textsf{"this books has two authors" ("este libro tiene dos autores")} \newline
\begin{figure}[H]%con el [H] le obligamos a situar aquí la figura
\centering
\includegraphics[scale=0.40]{constituency.png}  %el parámetro scale permite agrandar o %achicar la imagen. En el nombre de archivo puede especificar directorios
\label{}
\caption{Posible árbol de gramática Constituyente}   
\end{figure}
\begin{figure}[H]%con el [H] le obligamos a situar aquí la figura
\centering
\includegraphics[scale=0.35]{dependency.png}  %el parámetro scale permite agrandar o %achicar la imagen. En el nombre de archivo puede especificar directorios
\label{}
\caption{Posible árbol de gramática de Dependencias}   
\end{figure}

 El parseo sintáctico una tarea computacionalmente muy intensiva, por lo que a veces es deseable sustituirla por otra menos costosa en cómputo que produzca resultados similares, como \textsf{localizar patrones textuales}. Sin embargo, esta sustitución no siempre es posible, dado que deben de tenerse fuentes de información semiestructuradas, estructuradas, o generadas por una máquina. \newline
 Algunas herramientas para parseo sintáctico son \textsf{StandfordParser}, \textsf{MaltParser} \citet{nivre2006maltparser} o \textsf{TurboParser} \url{http://www.cs.cmu.edu/~ark/TurboParser/}, estas dos últimas implementan gramáticas de dependencias.
\\[\baselineskip]
Aquí concluye la parte genérica del pipeline. Además del pipeline genérico, hay procesos que se engloban en una parte que se conoce como pipeline dependiente del dominio, algunos de los cuales se detallan en la siguiente sección.

\section{El pipeline de OpenNLP}


 
  



 