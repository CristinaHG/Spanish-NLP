%!TEX root = ../dissertation.tex
\chapter{Introducción y motivación}
\label{introducción}
\section{Introducción al PLN}
El proyecto desarrollado se engloba en el campo del Procesamiento del lenguaje natural. El lenguaje natural es cualquier lenguaje usado por los humanos para comunicarse (Alemán, Inglés, Español, Hindi...). Dado que estos lenguajes se transmiten entre generaciones y van experimentando evoluciones, resulta difícil obtener reglas que los describan. (PLN) es, por tanto, el área de estudio y aplicación que engloba cualquier tipo de manipulación computacional del lenguaje natural.\newline Es decir, PNL abarca desde aplicaciones simples, como contar el número de ocurrencias de las palabras en un texto para comparar diferentes estilos de escritura, a aplicaciones más complejas, como comprender expresiones humanas completas para poder dar respuestas útiles a preguntas. \citet{bird2009natural} Como por ejemplo, el asistente Siri de iPhone. 

La lingüística computacional o PLN comenzó en 1980, sin embargo en los últimos 20 años ha crecido enormemente, despertando un gran interés en el ámbito de la investigación científica pero también en el ámbito práctico, ya que cada vez son más los productos, especialmente los tecnológicos, que incorporan algún tipo de aplicación basada en NLP. Por ejemplo, traductores como el traductor de Skype, o asistentes de voz inteligentes (Cortana de Microsoft, Google Now de Google o el ya mencionado Siri de Apple). \newline
Este crecimiento en el campo del procesamiento del lenguaje natural se debe principalmente a que en los últimos años, con el uso de redes sociales como Facebook, SnapChat, Twitter, Google plus, Linked in... y de sitios web comerciales como Amazon o Booking, los usuarios han generado una gran cantidad de contenido mayoritariamente subjetivo, el cual se puede aplicar en muchos ámbitos como márketing, política, gestión de crisis, soporte, atención al cliente, etc. También han influido en su crecimiento el aumento de capacidad de procesamiento y cómputo que ha habido en los últimos años y el desarrollo de técnicas de machine learning más complejas y potentes.    
\newline

En la actualidad, según lo descrito en \citet{hirschberg2015advances} éstas son algunas de las principales áreas en PLN:
\section*{Traducción automática}
La traducción automática es el área del PLN que tiene como objetivo el empleo de sofware para ayudar a traducir de un lenguaje natural a otro, ya sea en texto o hablado. Ésto supone una gran dificultad, ya que para que una traducción sea correcta, no basta con traducir palabra a palabra, sino que hay que tener en cuenta el sentido de la palabra y el contexto de ésta, pues hay casos en los que la misma palabra significa varias cosas dependiendo del contexto. Por ejemplo, en las frases \textsl{"compra una lata de refresco"} y \textsl{"deja ya de dar la lata"} aparece la  palabra \textsl{lata} desempeñando una  función distinta:\newline En la primera frase, \textsl{lata} es un nombre, por lo que se entendería como un envase hecho de hojalata, mientras que en la segunda frase aparece como una locución verbal, por lo que se entendería como "molestar" o "importunar". \newline  
El campo de la traducción automática se empezó a estudiar a finales de 1950s, sin embargo inicialmente no tuvo mucho éxito debido a que los traductores construidos eran sistemas basados en gramáticas escritas a mano. Fue a partir de 1990 y gracias a que los científicos de IBM consiguieron una cantidad suficientemente grande de frases de traducciones entre dos lenguajes, cuando construyeron un modelo probabilístico de traducción automática.\newline
A partir de entonces se siguió investigando y se descubrieron los \textcolor{SchoolColor}{traductores máquina basados en frases}, que en lugar de ir traduciendo palabra a palabra, detectaban los pequeños subgrupos de palabras que solían ir juntas y que tenían una traducción especial. Esto se utilizó para desarrollar el traductor de Google.  
\newline
Actualmente, el estado del arte en este campo está en traductores máquinas que usan deep learning, entrenando un modelo de varios niveles para optimizar un objetivo (la calidad de la traducción), donde luego el modelo pueda aprender por sí mismo más niveles que le sean útiles para desarrollar la tarea. Esto ha sido estudiado especialmente para redes neuronales, habiendo conseguido en varios casos obtener los mejores resultados hasta el momento, empleando redes neuronales distribuidas. Como por ejemplo, en  \citet{luong2014addressing}.

\section*{Sistemas de reconocimiento del habla}

Esta área, muy conocida desde 1980s, estudia como permitir y mejorar la comunicación entre humanos y máquinas. Aunque siempre se ha pensado, por ejemplo, en aplicaciones como robots que ayudan en casa o a personas con movilidad reducida,no muchos años atrás se expandió al ámbito de los smartphones (mencionábamos en la introducción a los asistentes de voz para móvil más conocidos). \newline
El reconocimiento del habla necesita principalmente de :
\begin{itemize}
\item Una herramienta de reconocimiento automático del habla (RAH) para identificar que está diciendo el humano.
\item Una herramienta de manejo de diálogo (MD) para identificar lo que quiere el humano.
\item Acciones para realizar la actividad solicitada.
\item Una herramienta de síntesis texto a voz para que la máquina pueda comunicar al humano el resultado de forma hablada.
\end{itemize}
Sin embargo, aún se está investigando como hacer estas herramientas más precisas. Añadiéndole a lo anterior las dificultades propias de reconocer lenguaje humano hablado: pausas, coletillas, coordinación, toma de turnos... desemboca en que los sistemas de reconocimiento de habla aún no han tenido gran éxito interactuando en dominios abiertos, donde los usuarios pueden hablar de cualquier cosa, aunque en dominios cerrados donde conocían el tema han mostrado resultados mejores. \newline
En los últimos años se ha aplicado deep learning en estos sistemas, mapeando señales de sonido a secuencias de palabras y sonidos del lenguaje humano \citet{hinton2012deep}, aunque actualmente el
enfoque más usado es el proceso de decisión de Markov, que hace identificación del diálogo (pregunta, sentencia, acuerdo..) mediante una probabilidad de distribución sobre todos los posibles estados del sistema, que va actualizando según se desarrolla el diálogo. \citet{young2013pomdp}.

\section*{Lectura Automática}
La lectura automática es el área que tiene como objetivo que las máquinas puedan integrar o resumir información a los humanos, mediante la lectura y comprensión de las grandes cantidades de texto disponibles. \newline
Esta idea atrae especialmente a los científicos, ya que es complicado llevar el ritmo de todas las publicaciones que se hacen, aunque sólo sea en su campo, por lo que sería de gran utilidad que un sistema pudiera resumir e identificar los datos más relevantes de las publicaciones. 
El objetivo inicial de estos sistemas es la extracción de relaciones, es decir, ser capaz de extraer relaciones entre dos entidades, como por ejemplo "A es hermano de B", lo cual ya se ha realizado con éxito en dominios específicos. Aunque hay técnicas que escriben los patrones de las relaciones a mano (por ejemplo: <PERSONA>, el hermano de <PERSONA>), se ha demostado que aplicando Machine learning se obtienen mejores resultados, ya que se pueden obtener relaciones basadas en características extraidas de secuencias de palabras y secuencias gramaticales de una frase. \citet{culotta2004dependency}.
\newline
Los sistemas más recientes han usado inferencia probabilística sofisticada para distinguir qué claúsulas textuales se asocian a qué factores de la base del conocimiento, por ejemplo,  \citet{niu2012deepdive} y apuestan por técnicas de extracción de hechos más simples pero más escalables que no requieren etiquetado manual de los datos, o las extraen usando NLP. \citet{etzioni2011open}. 

     
\section*{Minería de datos en medios sociales}
La minería de datos es el campo que tiene como objetivo descubrir patrones en grandes volúmenes de datos. Hoy en día, la gran cantidad de datos disponibles a través de redes sociales (Facebook, Twitter, Instagram, Youtube..), blogs o foros se puede descargar usando técnicas de web scrapping y se usa, aplicando técnicas de machine learning e inteligencia artificial, para aprender a detectar información demográfica a partir del lenguaje (como sexo o edad), hacer un seguimiento de las tendencias más populares u opiniones más populares sobre política o sobre productos, e incluso, como hizo Google (\url{www.google.org/flutrends/}) para ver como se difunde la gripe a través de los tweets de los usuarios y sus búsquedas en internet \citet{elhadad2014information}. \newline
A pesar de que este campo tiene innumerables aplicaciones, muchas de las cuales podrían ser de gran interés (como por ejemplo, detectar grupos que hacen bullying a otros o fomentan el odio), están aumentando los poblemas de privacidad y se está limitando el acceso a esos datos. Por ejemplo, Twitter          ya ha limitado el periodo de tiempo del que se pueden descargar tweets. Instagram también a modificado su API con este propósito. \newline
Otra dificultad  con la que cuenta este campo, es la validación. Muchas veces no hay forma de comprobar que la información presente en internet es cierta, por ejemplo las reseñas sobre hoteles, productos o restaurantes. En la actualidad, Facebook está ideando un modelo para detectar noticias falsas en su red social. Aunque se ha probado a agregar información de distintas fuentes para intentar validar la información, de momento no ha tenido mucho éxito.    
\section*{Análisis de sentimientos}
Este campo (también conocido como minería de opiniones) analiza las opiniones, sentimientos, valoraciones, actitudes y emociones de la gente frente a entidades como productos, servicios, organizaciones, individuos, eventos, temas, cuestiones...\newline
\citet{9781107017894} emplea el término \textsl{opinión} para referirse al concepto de sentimento, evaluación, valoración o actitud e información asociada (objetivo de la opinión o persona que da la opinión) en su totalidad, y el término \textsl{sentimiento} para referirse al sentido positivo o negativo subyacente en una opinión. Por ejemplo \textsl{"Apple lo está haciendo muy bien en esta economía pobre"} es una opinión que contiene dos sentimientos, uno positivo con Apple como objetivo y otro negativo sobre la economía actual.\newline
 Los estudios sobre este campo comenzaron en el año 2000, principalmente debido a que para entonces se empezó a recoger texto subjetivo en formato digital. Actualmente hay muchos campos relacionados con este cuyas tareas difieren ligeramente, por ejemplo análisis de opiniones, análisis de subjetividad, minería de sentimientos... aunque gran parte del trabajo se concentra en el análisis de sentimientos. \newline
 Los enfoques más simples tratan de identificar si lo expresado en el texto (por ejemplo, en un tweet) tiene una orientación positiva o negativa usando dicccionarios de sentimientos como \citet{whissell1989dictionary}. Otros enfoques más complejos tratan de identificar la polaridad del sentimiento así como el objeto de éste.\citet{wiebe2005annotating}. También se han realizado trabajos recientes tratando de indentificar algunas emociones en particular, como las de Ekman (furia, aversión, miedo, felicidad,tristeza y sorpresa) y se ha investigado sobre reconocer esas emociones clásicas usando características como la edad, la personalidad , el género las condiciones mentales o médicas del usuario.  \citet{hirschberg2015advances}. \newline
Las aplicaciones de este campo son innumerables y abarcan desde identificar valoraciones en productos \citet{wang2015sentiment} a predecir los precios del mercado o evaluar el estado mental de una comunidad. \citet{bollen2011modeling}.
\section{Historia del PLN}
De acuerdo a lo descrito en \citet{hirschberg2015advances}, el PLN comienza en 1980s como intersección entre la inteligencia artificial y la lingüística..  
Durante las primeras décadas, los investigadores escribían a mano las reglas y el vocabulario del lenguaje humano. Sin embargo, no se obtuvo éxito, debido a la variabilidad y dificultad del lenguaje humano. Por ejemplo, el traductor palabra a palabra de ruso a inglés que no tenía en cuenta el contexto, el léxico o la morfología y que tradujo la frase bíblica : \textsl{"El espíritu está dispuesto, pero la carne es débil"} como \textsl{"El vodka es agradable, pero la carne es
estropeado"} según \citet{nadkarni2011natural}. \newline
Es a partir de 1990 cuando el PLN sufre una transformación cuando los investigadores comienzan a tener la posibilidad de obtener grandes cantidades de datos del lenguaje en formado digital y construyen modelos sobre estos. Surge así el PLN estadístico o PLN basado en corpus, lo que supuso un éxito en el uso del "big data", aunque ese término se introduciría más adelante. Con lo anterior surgen métodos que usan el \textsc{Part-Of-Speech} (POS) de las palabras (es decir, si son sustantivo, un adjetivo, un verbo, una preposición...) adquiriendo notables resultados cuando se entrena con un conjunto de datos suficientemente grande. \newline
Actualmente, muchos clasificadores de texto y sentimientos se basan únicamente en los diferentes conjuntos de palabras que presenta el texto (bolsas de palabras) sin tener en cuenta estructuras a nivel de frase, de documento o de significado. Sin embargo, los mejores enfoques actuales usan técnicas sofisticadas de machine learning y un buen entendimiento de la estructura lingüística subyacente, identificando información sintáctica, semántica y de contexto. Por ejemplo, Standford CoreNLP \citet{manning2014stanford} o python NLTK \citet{bird2006nltk}. 

\section{El pipeline Genérico}
El objetivo final de toda suite de PLN es tener un herramienta para poder realizar análisis de sentimientos o algún tipo de extracción de información. Esto se alcanza a través de varias tareas de procesamiento del texto, empezando por hacer el contenido uniforme y acabando por identificar las funciones de las palabras y como se organizan. Aquí se describen las tareas más comunes del pipeline de un software de PLN junto con las aproximaciones más comunes. \newline
\subsection{Obtención de datos}
La obtención de datos es la primera tarea a realizar, mediante la cual se obtendrá el corpus de texto, etiquetado o no (depende del enfoque ver sección tal) que se va a usar para crear la herramienta de PLN, así como los datos sobre los que se quiere hacer el análisis de sentimientos o la extracción de información deseada. \newline
La forma general de obtener los datos directamente de internet es a través de la propia API de los sitios web de los que se desee descargar información por ejemplo la Api de twitter si queremos descargar tweets, o la api de instagram para imágenes y etiquetas. Sin embargo, a pesar de que hay manuales que explican como consultarlas, estas APIs  han limitado mucho las descargas últimamente, alegando cuestiones de privacidad de los datos de usuario. Otro método es usar arañas web para rastrear la información deseada de forma similar a lo que hace google con su conocida araña \textsc{Googlebot}, con la que visita todas las webs frecuentemente para añadirlas a su índice y percatarse de los cambios. Ahora bien, este último método requiere unos conocimientos necesarios para crear una araña web y ocupa mucho ancho de banda. \newline
En cuanto a corpus ya etiquetados en inglés se pueden encontrar muchos, entre los más populares están \textsc{The Penn Treebank} \citet{marcus1993building} o \textsc{Wikicorpus} \url{http://www.cs.upc.edu/~nlp/wikicorpus/}, mientras que para español son conocidos \textsc{Wikicorpus}, \textsc{Ancora} \url{http://clic.ub.edu/ancora} o \textsc{SemEval} \citet{marquez2007semeval}. 
    
\subsection{Procesamiento del texto}
Teniendo el texto, se le aplica un procesamiento, separándolo en unidades relevantes, añadiendo información sintáctica y morfológica, extrayendo entidades y relaciones entre estas... En este apartado se describen las fases de procesamiento más comunes según describe \citet{9783319155623}.
\subsubsection{Tokenización}
En esta fase se separa el texto del documento en sus unidades atómicas, los tokens (palabras, números, símbolos), por lo que es una fase totalmente necesaria en casi cualquier suite de PLN. Aunque no es una tarea muy compleja para lenguajes que emplean espacios entre palabras, como la mayoría de lenguajes que usan el alfabeto latino, sí que resulta mucho más complicado en lenguajes que no usan espacios entre palabras, como el Chino. \citet{chang2008optimizing}. \newline
La tokenización se sirve de heurísticas simples, como considerar que todos los strings de caracteres del alfabeto contiguos forman parte del mismo token (igualmente para los números) y que todos los tokens van separados unos de otros por espacios, saltos de línea o por signos de puntuación que no sean abreviaciones. Algunas herramientas actuales que hacen tokenización son \textsf{Freeling} \citet{padro2012freeling}, \textsf{Apache OpenNLP} \citet{baldridge2005opennlp} y los ya citados \textsf{NLTK} y \textsf{StandfordNLP}. No hay ninguna herramienta especialmente dedicada a tokenizar, ya que la tokenización puede ser razonablemente bien hecha usando expresiones regulares cuando se procesan lenguajes que usan el alfabeto latino. Para otros lenguajes más complejos, como el Árabe o el Chino, sí se precisan de tokenizadores más complejos, por ejemplo, el \textsf{Standford Word Segmenter}  es un tokenizador para estos lenguajes que aplica segmentación de palabras usando CRFs (Conditional Random Fields). 
\subsubsection{Detección de límites de oraciones}
En esta fase se aborda el problema de delimitar los límites de una oración en el texto. En algunos casos esto se incluye dentro de la fase de tokenización. Encontrar los límites de una oración no es una tarea trivial, ya que las marcas de puntuación que delimitan el final de una frase suelen ser ambiguas en muchos lenguajes. Por ejemplo, en español lo son, ya que el punto se puede usar como marcador de final de oración pero también como separador entre parte entera y decimal en números reales o en iniciales y abreviaciones (por ejemplo, Srta.). 
 




 