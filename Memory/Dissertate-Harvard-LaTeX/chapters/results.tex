%!TEX root = ../dissertation.tex
\chapter{Marco experimental y resultados}
\label{conclusion}

\section{Marco experimental} 
A continuación se dan unas pautas para probar las herramientas implementadas si se desea. Se ha intentado simplificar el procedimiento, por lo que se incluye un fichero con un object \textsf{ScalaApp} que implementa un método main en el que se incluyen las rutas de todos los paths de los ficheros que se usan ya especificados. 
Para probar la funcionalidad, hay que ejecutar \textcolor{SchoolColor}{ScalaApp.scala} especificando las siguientes opciones:
\begin{itemize}
\item Texto que se desea parsear, sin poner comillas, pues ya las pone el programa por el usuario.
\item Opciones que se desean aplicar separadas por comas. Es decir, si se quiere hacer hasta la lematización, se especificaría \textsf{true,true,true}. Si por el contrario se desea tokenizar y etiquetar pero no lematizar, se especificaría  \textsf{true,true,false}. Si se especifican menos de tres opciones, el programa toma las restantes como false atomáticamente.
\item Seleccionar la notación que queremos para las etiquetas. Hay que introducir un número entre 1 y 3, donde 1 representa el tagset parole, 2 representa el tagset penntreebank y 3 representa el tagset universal. Si se introduce otro número, lanzará una excepción con un mensaje: \texttt{"Introduzca un número válido [1-3]"}. 
\end{itemize}

\begin{figure}[H]%con el [H] le obligamos a situar aquí la figura
\centering
\includegraphics[scale=0.65]{ejecucion.png}  %el parámetro scale permite agrandar o %achicar la imagen. En el nombre de archivo puede especificar directorios
\label{}
\caption{Ejemplo de ejecución desde el main}   
\end{figure}

donde cada palabra aparece en su forma original introducida, junto con su etiqueta y su lema, separados por \textsf{/}. 
\section{Resultados obtenidos}
%singularize accuracy:0.9390681predicative accuracy:0.9468599accuracy lematization of verbs: 0.8082782accuracy %tagger:0.91993284
A continuación se representan los resultados finales obtenidos por los algoritmos implementados en este trabajo frente a los resultados obtenidos por esos métodos en trabajo tomado como referencia. No se incluye comparación para el tokenizador dado que no hay costumbre de comparar resultados de tokenizadores, pues suelen funcionar bastante bien en lenguajes espaciados entre palabras, como el nuestro. 
\begin{table}[H]
\centering
\caption*{Resultados de los algoritmos}
\label{my-label}
\begin{tabular}{l|l|l|llll}
\cline{2-3}
                       &    \textsf{Pattern.es}               &   \textsf{Este trabajo}                &  \\ \cline{1-3}
\multicolumn{1}{|l|}{singularize} & 0.9390681  & 0.9390681     &   \\ \cline{1-1}
\multicolumn{1}{|l|}{predicative} & 0.9323671  & 0.9468599     &    \\ \cline{1-1}
\multicolumn{1}{|l|}{find lemma}  & 0.8082355  & 0.8082782     &   \\ \cline{1-1}
\multicolumn{1}{|l|}{tagger}      & 0.9279443  & 0.9199328    &   \\ \cline{1-3}
\end{tabular}
\end{table}

Por lo general los resultados son muy similares, obteniendo exactamente el mismo resultado para el algoritmo de singularización y prácticamente el mismo para el algoritmo de etiquetado y lematización de verbos, siendo éste último implementado en este trabajo algo levemente superior. \newline
Quizá la mayor diferencia esté en el algoritmo predicativo, aunque tampoco es una diferencia alarmante, y en este caso, es a favor de nuestro trabajo. Esta diferencia se debe casi con toda seguridad a que se ha decidido etiquetar inicialmente las palabras desconocidas que no sean nombre propios ni números como \texttt{"NCS"} (nombre común en singular) en lugar de \texttt{"NN"} como hace \textsf{Pattern.es}. Cabe mencionar, que estos resultados se miden a nivel de palabra bien etiquetada, y no a nivel de oración, pues de esta forma se reduciría el acierto en los resultados casi a la mitad. El estado del arte en pos etiquetado y lematización está actualmente entre el 96\%-98\%, pero si se midiera como porcentaje de frases completas bien etiquetadas, quedaría entre un 50\%-57\%, por lo que realmente el problema está lejos de estar resuelto. 

\section{Ejemplos de errores}
A continuación se exponen algunos ejemplos de errores en el Pos etiquetado y lematización de \newline algunas oraciones: 
\subsection*{Ejemplo 1}
Para la frase de entrada \texttt{"Perro con hueso en la boca, ni muerde ni ladra."} se obtiene la salida: \newline

\noindent\rule{14cm}{0.4pt}\newline
\texttt{Perro/NN/Perro\newline
con/IN/con\newline
hueso/NN/hueso\newline
en/IN/en\newline
la/DT/el\newline
boca/NN/boca\newline
,/,/,\newline
ni/CC/ni\newline
muerde/VB/\textcolor{red}{muerder}\newline
ni/CC/ni\newline
ladra/\textcolor{red}{NN}/\textcolor{red}{ladra}\newline
././.\newline
}
\noindent\rule{14cm}{0.4pt}\newline

Se observa que se comete un fallo de lematización, ya que aunque etiqueta \texttt{muerde} correctamente como un verbo, lo lematiza como \texttt{\textcolor{red}{muerder}}, palabra que no existe. Esto se debe a que dentro del algoritmo de lematización, intenta aplicar una regla para formas acabadas en \texttt{"-e"} con inflexiones \newline regulares (\texttt{"comes","vive","ejerce","corre"...}), que consiste en quitar la última letra y añadir \texttt{"-er"} a la palabra, si ésta sin prefijo no es mayor que dos o no contiene una \texttt{"-i"} en antepenúltimo lugar. Otro error presente en la frase es etiquetar \texttt{ladra} como \texttt{\textcolor{red}{NN}}, lo que deriva en una lematización incorrecta de ésta palabra. \texttt{Ladra} inicialmente es desconocida (no está en el diccionario), por lo que se etiqueta inicialmente como tal. Posteriormente, el algoritmo aplica reglas morfológicas, pero no encuentra ninguna que encaje, por lo que la etiqueta con la etiqueta por defecto que le hemos dicho, y eso es \texttt{NCS}, que en formato penntreebank es \texttt{NN}.
\subsection*{Ejemplo 2}
Para la frase de entrada \texttt{"Los mosquitos tienen la mala costumbre de querer contarnos secretos al oído."} se obtiene la salida: \newline
\noindent\rule{14cm}{0.4pt}\newline
\texttt{Los/DT/el\newline
mosquitos/\textcolor{red}{NN}/\textcolor{red}{mosquitos}\newline
tienen/VB/tener\newline
la/DT/el\newline
mala/JJ/malo\newline
costumbre/NN/costumbre\newline
de/IN/de\newline
querer/VB/querer\newline
contarnos/\textcolor{red}{NN}/\textcolor{red}{contarnos}\newline
secretos/NNS/secreto\newline
al/NN/al\newline
oído/\textcolor{red}{VBN}/\textcolor{red}{oir}\newline
././.\newline
}
\noindent\rule{14cm}{0.4pt}\newline
En este caso encontramos tres errores: el primero en el POS etiquetado, al etiquetar la palabra \texttt{mosquitos} como nombre simple, derivando en error también en la lematización, pues se considera que si una palabra es un nombre singular, ésta está en su forma base. El segundo es etiquetar \texttt{contarnos} como \texttt{\textcolor{red}{NN}} en lugar de \texttt{VB}, debido a que es desconocida, por lo que se etiqueta por defecto, pero luego no encuentra regla morfológica o contextual que la empareje. El último error es etiquetar \texttt{oído} como \texttt{\textcolor{red}{VBN}}, lematizándolo como tal, cuando realiza la función de sustantivo. Debido al léxico, esta palabra se etiqueta inicialmente como \texttt{VMP}, por lo que no se le aplican reglas morfológicas \newline después ni encuentra reglas contextuales que cambien su etiqueta, quedándose finalmente como \texttt{VBN} al mapear a formato penntreebank. 
\subsection*{Ejemplo 3}
Para la frase de entrada \texttt{"En navidad siempre se juntan la abuela con la botella de anís, los niños correteando y el perro ladrando."} se obtiene la salida: \newline
\noindent\rule{14cm}{0.4pt}\newline
\texttt{En/IN/En\newline
navidad/NN/navidad\newline
siempre/RB/siempre\newline
se/PRP/se\newline
juntan/VB/juntar\newline
la/DT/el\newline
abuela/NN/abuela\newline
con/IN/con\newline
la/DT/el\newline
botella/NN/botella\newline
de/IN/de\newline
anís/NN/anís\newline
,/,/,\newline
los/DT/el\newline
niños/NNS/niño\newline
correteando/\textcolor{red}{NN}/\textcolor{red}{correteando}\newline
y/CC/y\newline
el/DT/el\newline
perro/NN/perro\newline
ladrando/\textcolor{red}{NN}/\textcolor{red}{ladrando}\newline
././.\newline
}
\noindent\rule{14cm}{0.4pt}\newline
Observamos dos errores de POS etiquetado que derivan de nuevo en dos errores de lematización. Para \texttt{ladrando} se aplica el mismo caso que en el ejemplo 1: es una palabra desconocida y no se encuentran reglas morfológicas ni contextuales que le apliquen, quedando etiquetada como un nombre, cuando en realidad es un verbo.
En segundo error es etiquetar \texttt{correteando} como nombre, por la misma razón que en el caso de \texttt{ladrando}.