%!TEX root = ../dissertation.tex
\chapter{Marco experimental y resultados}
\label{conclusion}

\section{Marco experimental} 
A continuación se dan unas pautas para probar las herramientas implementadas si se desea. Se ha intentado simplificar el procedimiento, por lo que se incluye un fichero con un object \textsf{ScalaApp} que implementa un método main en el que se incluyen las rutas de todos los paths de los ficheros que se usan ya especificados. 
Para probar la funcionalidad, hay que ejecutar \textcolor{SchoolColor}{ScalaApp.scala} especificando las siguientes opciones:
\begin{itemize}
\item Texto que se desea parsear, sin poner comillas, pues ya las pone el programa por el usuario.
\item Opciones que se desean aplicar separadas por comas. Es decir, si se quiere hacer hasta la lematización, se especificaría \textsf{true,true,true}. Si por el contrario se desea tokenizar y etiquetar pero no lemtizar, se especificaría  \textsf{true,true,false}. Si se especifican menos de tres opciones, el programa toma las restantes como false atomáticamente.
\item Seleccionar la notación que queremos para las etiquetas. Hay que introducir un número entre 1 y 3, donde 1 representa el tagset parole, 2 representa el tagset penntreebank y 3 representa el tagset universal. Si se introduce otro número, lanzará una excepción con un mensaje: \texttt{"Introduzca un número válido [1-3]"}. 
\end{itemize}

\begin{figure}[H]%con el [H] le obligamos a situar aquí la figura
\centering
\includegraphics[scale=0.65]{ejecucion.png}  %el parámetro scale permite agrandar o %achicar la imagen. En el nombre de archivo puede especificar directorios
\label{}
\caption{Ejemplo de ejecución desde el main}   
\end{figure}

donde cada palabra aparece en su forma original introducida, junto con su etiqueta y su lema, separados por \textsf{/}. 
\section{Resultados obtenidos}
%singularize accuracy:0.9390681predicative accuracy:0.9468599accuracy lematization of verbs: 0.8082782accuracy %tagger:0.91993284
A continuación se representan los resultados finales obtenidos por los algoritmos implementados en este trabajo frente a los resultados obtenidos por esos métodos en trabajo tomado como referencia. No se incluye comparación para el tokenizador dado que no hay costumbre de comparar resultados de tokenizadores, pues suelen funcionar bastante bien en lenguajes espaciados entre palabras, como el nuestro. 
\begin{table}[H]
\centering
\caption*{Resultados de los algoritmos}
\label{my-label}
\begin{tabular}{l|l|l|llll}
\cline{2-3}
                       &    \textsf{Pattern.es}               &   \textsf{Este trabajo}                &  \\ \cline{1-3}
\multicolumn{1}{|l|}{singularize} & 0.9390681  & 0.9390681     &   \\ \cline{1-1}
\multicolumn{1}{|l|}{predicative} & 0.9323671  & 0.9468599     &    \\ \cline{1-1}
\multicolumn{1}{|l|}{find lemma}  & 0.8082355  & 0.8082782     &   \\ \cline{1-1}
\multicolumn{1}{|l|}{tagger}      & 0.9279443  & 0.9199328    &   \\ \cline{1-3}
\end{tabular}
\end{table}

Por lo general los resultados son muy similares, obteniendo exactamente el mismo resultado para el algoritmo de singularización y prácticamente el mismo para el algoritmo de lematización de verbos y etiquetado,siendo el de este trabajo algo muy levemente superior. \newline
Quizá la mayor diferencia esté en el algoritmo predicativo, aunque tampoco es una diferencia alarmante, y en este caso, es a favor de nuestro trabajo. Ésta diferencia se debe casi con toda seguridad a que se ha decidido etiquetar inicialmente las palabras desconocidas que no sean nombre propios ni números como \texttt{"NCS"} (nombre común en singular) en lugar de \texttt{"NN"} como hace \textsf{Pattern.es}.     